{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = pd.read_csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.drop('species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 4, activation = 'relu', input_shape=[4,]))\n",
    "model.add(Dense(units = 3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 30 samples\n",
      "Epoch 1/300\n",
      "120/120 [==============================] - 1s 5ms/sample - loss: 1.3183 - accuracy: 0.0000e+00 - val_loss: 1.3585 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 1.3098 - accuracy: 0.0000e+00 - val_loss: 1.3502 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.3007 - accuracy: 0.0000e+00 - val_loss: 1.3419 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2922 - accuracy: 0.0000e+00 - val_loss: 1.3336 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.2838 - accuracy: 0.0000e+00 - val_loss: 1.3256 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2758 - accuracy: 0.0000e+00 - val_loss: 1.3176 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/300\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 1.2682 - accuracy: 0.0000e+00 - val_loss: 1.3100 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2609 - accuracy: 0.0000e+00 - val_loss: 1.3026 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.2533 - accuracy: 0.0083 - val_loss: 1.2954 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2464 - accuracy: 0.0250 - val_loss: 1.2883 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/300\n",
      "120/120 [==============================] - 0s 124us/sample - loss: 1.2398 - accuracy: 0.0333 - val_loss: 1.2815 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.2329 - accuracy: 0.0333 - val_loss: 1.2751 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.2265 - accuracy: 0.0417 - val_loss: 1.2687 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.2203 - accuracy: 0.0750 - val_loss: 1.2624 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2145 - accuracy: 0.0833 - val_loss: 1.2563 - val_accuracy: 0.0333\n",
      "Epoch 16/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.2083 - accuracy: 0.1083 - val_loss: 1.2504 - val_accuracy: 0.0333\n",
      "Epoch 17/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.2025 - accuracy: 0.1083 - val_loss: 1.2445 - val_accuracy: 0.0333\n",
      "Epoch 18/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1973 - accuracy: 0.1167 - val_loss: 1.2388 - val_accuracy: 0.0333\n",
      "Epoch 19/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1916 - accuracy: 0.1333 - val_loss: 1.2333 - val_accuracy: 0.0333\n",
      "Epoch 20/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1863 - accuracy: 0.1417 - val_loss: 1.2279 - val_accuracy: 0.0667\n",
      "Epoch 21/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.1811 - accuracy: 0.1667 - val_loss: 1.2227 - val_accuracy: 0.0667\n",
      "Epoch 22/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.1761 - accuracy: 0.1833 - val_loss: 1.2175 - val_accuracy: 0.1000\n",
      "Epoch 23/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.1713 - accuracy: 0.2083 - val_loss: 1.2124 - val_accuracy: 0.1000\n",
      "Epoch 24/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.1664 - accuracy: 0.2333 - val_loss: 1.2076 - val_accuracy: 0.1333\n",
      "Epoch 25/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1619 - accuracy: 0.2417 - val_loss: 1.2030 - val_accuracy: 0.1333\n",
      "Epoch 26/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1572 - accuracy: 0.2583 - val_loss: 1.1982 - val_accuracy: 0.1333\n",
      "Epoch 27/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1528 - accuracy: 0.2583 - val_loss: 1.1936 - val_accuracy: 0.1333\n",
      "Epoch 28/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1484 - accuracy: 0.2750 - val_loss: 1.1891 - val_accuracy: 0.1667\n",
      "Epoch 29/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1442 - accuracy: 0.2917 - val_loss: 1.1848 - val_accuracy: 0.2000\n",
      "Epoch 30/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1402 - accuracy: 0.3083 - val_loss: 1.1806 - val_accuracy: 0.2000\n",
      "Epoch 31/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1359 - accuracy: 0.3083 - val_loss: 1.1764 - val_accuracy: 0.2000\n",
      "Epoch 32/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.1319 - accuracy: 0.3083 - val_loss: 1.1725 - val_accuracy: 0.2000\n",
      "Epoch 33/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 1.1280 - accuracy: 0.3167 - val_loss: 1.1685 - val_accuracy: 0.2000\n",
      "Epoch 34/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1241 - accuracy: 0.3250 - val_loss: 1.1645 - val_accuracy: 0.2000\n",
      "Epoch 35/300\n",
      "120/120 [==============================] - 0s 158us/sample - loss: 1.1203 - accuracy: 0.3250 - val_loss: 1.1605 - val_accuracy: 0.2333\n",
      "Epoch 36/300\n",
      "120/120 [==============================] - 0s 191us/sample - loss: 1.1164 - accuracy: 0.3250 - val_loss: 1.1566 - val_accuracy: 0.2333\n",
      "Epoch 37/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 1.1128 - accuracy: 0.3250 - val_loss: 1.1527 - val_accuracy: 0.2667\n",
      "Epoch 38/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1091 - accuracy: 0.3250 - val_loss: 1.1489 - val_accuracy: 0.2667\n",
      "Epoch 39/300\n",
      "120/120 [==============================] - 0s 150us/sample - loss: 1.1056 - accuracy: 0.3250 - val_loss: 1.1452 - val_accuracy: 0.2667\n",
      "Epoch 40/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.1019 - accuracy: 0.3333 - val_loss: 1.1415 - val_accuracy: 0.2667\n",
      "Epoch 41/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0984 - accuracy: 0.3333 - val_loss: 1.1380 - val_accuracy: 0.2667\n",
      "Epoch 42/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0949 - accuracy: 0.3417 - val_loss: 1.1344 - val_accuracy: 0.2667\n",
      "Epoch 43/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0915 - accuracy: 0.3417 - val_loss: 1.1308 - val_accuracy: 0.2667\n",
      "Epoch 44/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.0880 - accuracy: 0.3417 - val_loss: 1.1274 - val_accuracy: 0.2667\n",
      "Epoch 45/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 1.0847 - accuracy: 0.3417 - val_loss: 1.1239 - val_accuracy: 0.2667\n",
      "Epoch 46/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0813 - accuracy: 0.3417 - val_loss: 1.1205 - val_accuracy: 0.2667\n",
      "Epoch 47/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0780 - accuracy: 0.3417 - val_loss: 1.1171 - val_accuracy: 0.2667\n",
      "Epoch 48/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0748 - accuracy: 0.3417 - val_loss: 1.1139 - val_accuracy: 0.2667\n",
      "Epoch 49/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 1.0716 - accuracy: 0.3417 - val_loss: 1.1106 - val_accuracy: 0.2667\n",
      "Epoch 50/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0684 - accuracy: 0.3417 - val_loss: 1.1075 - val_accuracy: 0.2667\n",
      "Epoch 51/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0653 - accuracy: 0.3417 - val_loss: 1.1045 - val_accuracy: 0.2667\n",
      "Epoch 52/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 1.0622 - accuracy: 0.3500 - val_loss: 1.1016 - val_accuracy: 0.2667\n",
      "Epoch 53/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0592 - accuracy: 0.3500 - val_loss: 1.0988 - val_accuracy: 0.2667\n",
      "Epoch 54/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0562 - accuracy: 0.3500 - val_loss: 1.0962 - val_accuracy: 0.2667\n",
      "Epoch 55/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0533 - accuracy: 0.3500 - val_loss: 1.0936 - val_accuracy: 0.2667\n",
      "Epoch 56/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0504 - accuracy: 0.3500 - val_loss: 1.0912 - val_accuracy: 0.3000\n",
      "Epoch 57/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0476 - accuracy: 0.3500 - val_loss: 1.0888 - val_accuracy: 0.3000\n",
      "Epoch 58/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0447 - accuracy: 0.3500 - val_loss: 1.0866 - val_accuracy: 0.3000\n",
      "Epoch 59/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0421 - accuracy: 0.3500 - val_loss: 1.0846 - val_accuracy: 0.3000\n",
      "Epoch 60/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0396 - accuracy: 0.3583 - val_loss: 1.0826 - val_accuracy: 0.3000\n",
      "Epoch 61/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0372 - accuracy: 0.3583 - val_loss: 1.0806 - val_accuracy: 0.3000\n",
      "Epoch 62/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0348 - accuracy: 0.3583 - val_loss: 1.0786 - val_accuracy: 0.3000\n",
      "Epoch 63/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 1.0326 - accuracy: 0.3667 - val_loss: 1.0766 - val_accuracy: 0.3000\n",
      "Epoch 64/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0303 - accuracy: 0.3667 - val_loss: 1.0747 - val_accuracy: 0.3000\n",
      "Epoch 65/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0282 - accuracy: 0.3750 - val_loss: 1.0729 - val_accuracy: 0.3000\n",
      "Epoch 66/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0261 - accuracy: 0.3750 - val_loss: 1.0709 - val_accuracy: 0.3000\n",
      "Epoch 67/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0239 - accuracy: 0.3750 - val_loss: 1.0691 - val_accuracy: 0.3000\n",
      "Epoch 68/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0218 - accuracy: 0.3750 - val_loss: 1.0673 - val_accuracy: 0.3000\n",
      "Epoch 69/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0197 - accuracy: 0.3833 - val_loss: 1.0656 - val_accuracy: 0.3000\n",
      "Epoch 70/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0176 - accuracy: 0.3833 - val_loss: 1.0637 - val_accuracy: 0.3000\n",
      "Epoch 71/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0155 - accuracy: 0.3833 - val_loss: 1.0618 - val_accuracy: 0.3000\n",
      "Epoch 72/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0135 - accuracy: 0.3833 - val_loss: 1.0601 - val_accuracy: 0.3000\n",
      "Epoch 73/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 1.0114 - accuracy: 0.3833 - val_loss: 1.0584 - val_accuracy: 0.3000\n",
      "Epoch 74/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0093 - accuracy: 0.3833 - val_loss: 1.0565 - val_accuracy: 0.3000\n",
      "Epoch 75/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0072 - accuracy: 0.3833 - val_loss: 1.0547 - val_accuracy: 0.3000\n",
      "Epoch 76/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 1.0051 - accuracy: 0.3833 - val_loss: 1.0529 - val_accuracy: 0.3000\n",
      "Epoch 77/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0030 - accuracy: 0.3833 - val_loss: 1.0511 - val_accuracy: 0.3000\n",
      "Epoch 78/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 1.0009 - accuracy: 0.3833 - val_loss: 1.0492 - val_accuracy: 0.3000\n",
      "Epoch 79/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9988 - accuracy: 0.3833 - val_loss: 1.0473 - val_accuracy: 0.3000\n",
      "Epoch 80/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9966 - accuracy: 0.3833 - val_loss: 1.0455 - val_accuracy: 0.3000\n",
      "Epoch 81/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9944 - accuracy: 0.3833 - val_loss: 1.0437 - val_accuracy: 0.3000\n",
      "Epoch 82/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9922 - accuracy: 0.3833 - val_loss: 1.0419 - val_accuracy: 0.3000\n",
      "Epoch 83/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9901 - accuracy: 0.3833 - val_loss: 1.0400 - val_accuracy: 0.3000\n",
      "Epoch 84/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9878 - accuracy: 0.3833 - val_loss: 1.0382 - val_accuracy: 0.3000\n",
      "Epoch 85/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9857 - accuracy: 0.3833 - val_loss: 1.0363 - val_accuracy: 0.3000\n",
      "Epoch 86/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9834 - accuracy: 0.3833 - val_loss: 1.0343 - val_accuracy: 0.3000\n",
      "Epoch 87/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9812 - accuracy: 0.3833 - val_loss: 1.0323 - val_accuracy: 0.3000\n",
      "Epoch 88/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.9788 - accuracy: 0.3833 - val_loss: 1.0304 - val_accuracy: 0.3000\n",
      "Epoch 89/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9766 - accuracy: 0.3917 - val_loss: 1.0284 - val_accuracy: 0.3000\n",
      "Epoch 90/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9743 - accuracy: 0.3917 - val_loss: 1.0263 - val_accuracy: 0.3000\n",
      "Epoch 91/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9720 - accuracy: 0.3917 - val_loss: 1.0244 - val_accuracy: 0.3000\n",
      "Epoch 92/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9697 - accuracy: 0.3917 - val_loss: 1.0224 - val_accuracy: 0.3000\n",
      "Epoch 93/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9673 - accuracy: 0.3917 - val_loss: 1.0203 - val_accuracy: 0.3333\n",
      "Epoch 94/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9650 - accuracy: 0.3917 - val_loss: 1.0182 - val_accuracy: 0.3333\n",
      "Epoch 95/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.9264 - accuracy: 0.46 - 0s 125us/sample - loss: 0.9627 - accuracy: 0.3917 - val_loss: 1.0163 - val_accuracy: 0.3333\n",
      "Epoch 96/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9602 - accuracy: 0.3917 - val_loss: 1.0142 - val_accuracy: 0.3333\n",
      "Epoch 97/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9579 - accuracy: 0.3917 - val_loss: 1.0121 - val_accuracy: 0.3333\n",
      "Epoch 98/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9556 - accuracy: 0.3917 - val_loss: 1.0100 - val_accuracy: 0.3333\n",
      "Epoch 99/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9531 - accuracy: 0.3917 - val_loss: 1.0079 - val_accuracy: 0.3333\n",
      "Epoch 100/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9507 - accuracy: 0.3917 - val_loss: 1.0059 - val_accuracy: 0.3333\n",
      "Epoch 101/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.9483 - accuracy: 0.3917 - val_loss: 1.0039 - val_accuracy: 0.3333\n",
      "Epoch 102/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9460 - accuracy: 0.3917 - val_loss: 1.0018 - val_accuracy: 0.3333\n",
      "Epoch 103/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9435 - accuracy: 0.3917 - val_loss: 0.9997 - val_accuracy: 0.3333\n",
      "Epoch 104/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9412 - accuracy: 0.3917 - val_loss: 0.9977 - val_accuracy: 0.3333\n",
      "Epoch 105/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9387 - accuracy: 0.3917 - val_loss: 0.9956 - val_accuracy: 0.3333\n",
      "Epoch 106/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9362 - accuracy: 0.3917 - val_loss: 0.9935 - val_accuracy: 0.3333\n",
      "Epoch 107/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9339 - accuracy: 0.3917 - val_loss: 0.9915 - val_accuracy: 0.3333\n",
      "Epoch 108/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9315 - accuracy: 0.3917 - val_loss: 0.9895 - val_accuracy: 0.3333\n",
      "Epoch 109/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9291 - accuracy: 0.3917 - val_loss: 0.9875 - val_accuracy: 0.3333\n",
      "Epoch 110/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9267 - accuracy: 0.3917 - val_loss: 0.9856 - val_accuracy: 0.3333\n",
      "Epoch 111/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.9245 - accuracy: 0.3917 - val_loss: 0.9838 - val_accuracy: 0.3000\n",
      "Epoch 112/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9221 - accuracy: 0.3917 - val_loss: 0.9818 - val_accuracy: 0.3000\n",
      "Epoch 113/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9198 - accuracy: 0.3917 - val_loss: 0.9798 - val_accuracy: 0.3000\n",
      "Epoch 114/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9175 - accuracy: 0.3917 - val_loss: 0.9779 - val_accuracy: 0.3000\n",
      "Epoch 115/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9151 - accuracy: 0.3917 - val_loss: 0.9758 - val_accuracy: 0.3000\n",
      "Epoch 116/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9128 - accuracy: 0.4167 - val_loss: 0.9738 - val_accuracy: 0.4333\n",
      "Epoch 117/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9105 - accuracy: 0.5000 - val_loss: 0.9718 - val_accuracy: 0.4333\n",
      "Epoch 118/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.9081 - accuracy: 0.5083 - val_loss: 0.9697 - val_accuracy: 0.4333\n",
      "Epoch 119/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.9058 - accuracy: 0.5167 - val_loss: 0.9677 - val_accuracy: 0.4333\n",
      "Epoch 120/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.9035 - accuracy: 0.5250 - val_loss: 0.9657 - val_accuracy: 0.4333\n",
      "Epoch 121/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.9013 - accuracy: 0.5417 - val_loss: 0.9637 - val_accuracy: 0.4333\n",
      "Epoch 122/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8989 - accuracy: 0.5667 - val_loss: 0.9617 - val_accuracy: 0.4333\n",
      "Epoch 123/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8966 - accuracy: 0.5833 - val_loss: 0.9596 - val_accuracy: 0.4333\n",
      "Epoch 124/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8943 - accuracy: 0.6083 - val_loss: 0.9575 - val_accuracy: 0.4333\n",
      "Epoch 125/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8920 - accuracy: 0.6167 - val_loss: 0.9555 - val_accuracy: 0.4333\n",
      "Epoch 126/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8898 - accuracy: 0.6333 - val_loss: 0.9536 - val_accuracy: 0.4333\n",
      "Epoch 127/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8874 - accuracy: 0.6333 - val_loss: 0.9515 - val_accuracy: 0.4333\n",
      "Epoch 128/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8852 - accuracy: 0.6333 - val_loss: 0.9494 - val_accuracy: 0.4667\n",
      "Epoch 129/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8830 - accuracy: 0.6417 - val_loss: 0.9474 - val_accuracy: 0.4667\n",
      "Epoch 130/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8807 - accuracy: 0.6417 - val_loss: 0.9454 - val_accuracy: 0.5000\n",
      "Epoch 131/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8784 - accuracy: 0.6500 - val_loss: 0.9434 - val_accuracy: 0.5000\n",
      "Epoch 132/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8761 - accuracy: 0.6583 - val_loss: 0.9414 - val_accuracy: 0.5000\n",
      "Epoch 133/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8739 - accuracy: 0.6583 - val_loss: 0.9393 - val_accuracy: 0.5333\n",
      "Epoch 134/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8718 - accuracy: 0.6667 - val_loss: 0.9375 - val_accuracy: 0.5333\n",
      "Epoch 135/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8695 - accuracy: 0.6750 - val_loss: 0.9354 - val_accuracy: 0.5333\n",
      "Epoch 136/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8674 - accuracy: 0.6833 - val_loss: 0.9335 - val_accuracy: 0.5333\n",
      "Epoch 137/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8652 - accuracy: 0.6833 - val_loss: 0.9314 - val_accuracy: 0.5333\n",
      "Epoch 138/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8629 - accuracy: 0.6917 - val_loss: 0.9294 - val_accuracy: 0.5333\n",
      "Epoch 139/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8608 - accuracy: 0.7000 - val_loss: 0.9274 - val_accuracy: 0.5333\n",
      "Epoch 140/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8587 - accuracy: 0.6917 - val_loss: 0.9256 - val_accuracy: 0.5333\n",
      "Epoch 141/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8565 - accuracy: 0.6917 - val_loss: 0.9236 - val_accuracy: 0.5333\n",
      "Epoch 142/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8544 - accuracy: 0.6917 - val_loss: 0.9216 - val_accuracy: 0.5667\n",
      "Epoch 143/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8523 - accuracy: 0.6917 - val_loss: 0.9197 - val_accuracy: 0.6000\n",
      "Epoch 144/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8501 - accuracy: 0.6917 - val_loss: 0.9176 - val_accuracy: 0.6000\n",
      "Epoch 145/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8480 - accuracy: 0.6917 - val_loss: 0.9155 - val_accuracy: 0.6000\n",
      "Epoch 146/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8460 - accuracy: 0.7000 - val_loss: 0.9136 - val_accuracy: 0.6000\n",
      "Epoch 147/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8438 - accuracy: 0.7000 - val_loss: 0.9117 - val_accuracy: 0.6000\n",
      "Epoch 148/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8417 - accuracy: 0.7000 - val_loss: 0.9095 - val_accuracy: 0.6000\n",
      "Epoch 149/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8397 - accuracy: 0.7000 - val_loss: 0.9076 - val_accuracy: 0.6333\n",
      "Epoch 150/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8375 - accuracy: 0.7000 - val_loss: 0.9055 - val_accuracy: 0.6333\n",
      "Epoch 151/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8354 - accuracy: 0.7000 - val_loss: 0.9034 - val_accuracy: 0.6333\n",
      "Epoch 152/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8334 - accuracy: 0.7000 - val_loss: 0.9013 - val_accuracy: 0.6333\n",
      "Epoch 153/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8313 - accuracy: 0.7000 - val_loss: 0.8993 - val_accuracy: 0.6333\n",
      "Epoch 154/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8292 - accuracy: 0.7000 - val_loss: 0.8973 - val_accuracy: 0.6333\n",
      "Epoch 155/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8272 - accuracy: 0.7167 - val_loss: 0.8953 - val_accuracy: 0.6333\n",
      "Epoch 156/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8251 - accuracy: 0.7167 - val_loss: 0.8933 - val_accuracy: 0.6333\n",
      "Epoch 157/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8231 - accuracy: 0.7167 - val_loss: 0.8914 - val_accuracy: 0.6333\n",
      "Epoch 158/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8210 - accuracy: 0.7167 - val_loss: 0.8894 - val_accuracy: 0.6333\n",
      "Epoch 159/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8191 - accuracy: 0.7167 - val_loss: 0.8874 - val_accuracy: 0.6333\n",
      "Epoch 160/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8171 - accuracy: 0.7167 - val_loss: 0.8856 - val_accuracy: 0.6333\n",
      "Epoch 161/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8150 - accuracy: 0.7167 - val_loss: 0.8835 - val_accuracy: 0.6333\n",
      "Epoch 162/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8130 - accuracy: 0.7167 - val_loss: 0.8813 - val_accuracy: 0.6333\n",
      "Epoch 163/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.8110 - accuracy: 0.7167 - val_loss: 0.8794 - val_accuracy: 0.6333\n",
      "Epoch 164/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8090 - accuracy: 0.7167 - val_loss: 0.8776 - val_accuracy: 0.6333\n",
      "Epoch 165/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.8070 - accuracy: 0.7167 - val_loss: 0.8756 - val_accuracy: 0.6333\n",
      "Epoch 166/300\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.7599 - accuracy: 0.75 - 0s 108us/sample - loss: 0.8050 - accuracy: 0.7167 - val_loss: 0.8736 - val_accuracy: 0.6333\n",
      "Epoch 167/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.8031 - accuracy: 0.7167 - val_loss: 0.8715 - val_accuracy: 0.6333\n",
      "Epoch 168/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.8011 - accuracy: 0.7167 - val_loss: 0.8695 - val_accuracy: 0.6333\n",
      "Epoch 169/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7991 - accuracy: 0.7167 - val_loss: 0.8673 - val_accuracy: 0.6333\n",
      "Epoch 170/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7972 - accuracy: 0.7167 - val_loss: 0.8653 - val_accuracy: 0.6333\n",
      "Epoch 171/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7952 - accuracy: 0.7167 - val_loss: 0.8633 - val_accuracy: 0.6333\n",
      "Epoch 172/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7932 - accuracy: 0.7167 - val_loss: 0.8612 - val_accuracy: 0.6333\n",
      "Epoch 173/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7913 - accuracy: 0.7167 - val_loss: 0.8591 - val_accuracy: 0.6333\n",
      "Epoch 174/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7894 - accuracy: 0.7167 - val_loss: 0.8571 - val_accuracy: 0.6667\n",
      "Epoch 175/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7874 - accuracy: 0.7167 - val_loss: 0.8551 - val_accuracy: 0.6667\n",
      "Epoch 176/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7855 - accuracy: 0.7167 - val_loss: 0.8530 - val_accuracy: 0.6667\n",
      "Epoch 177/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7835 - accuracy: 0.7250 - val_loss: 0.8511 - val_accuracy: 0.6667\n",
      "Epoch 178/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7816 - accuracy: 0.7250 - val_loss: 0.8492 - val_accuracy: 0.6667\n",
      "Epoch 179/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7797 - accuracy: 0.7250 - val_loss: 0.8473 - val_accuracy: 0.6667\n",
      "Epoch 180/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7778 - accuracy: 0.7250 - val_loss: 0.8454 - val_accuracy: 0.6667\n",
      "Epoch 181/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7759 - accuracy: 0.7250 - val_loss: 0.8436 - val_accuracy: 0.6667\n",
      "Epoch 182/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7740 - accuracy: 0.7250 - val_loss: 0.8416 - val_accuracy: 0.6667\n",
      "Epoch 183/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7721 - accuracy: 0.7250 - val_loss: 0.8397 - val_accuracy: 0.6667\n",
      "Epoch 184/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7702 - accuracy: 0.7250 - val_loss: 0.8377 - val_accuracy: 0.6667\n",
      "Epoch 185/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7683 - accuracy: 0.7250 - val_loss: 0.8358 - val_accuracy: 0.6667\n",
      "Epoch 186/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.7665 - accuracy: 0.7250 - val_loss: 0.8340 - val_accuracy: 0.6667\n",
      "Epoch 187/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7646 - accuracy: 0.7250 - val_loss: 0.8319 - val_accuracy: 0.6667\n",
      "Epoch 188/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7627 - accuracy: 0.7250 - val_loss: 0.8300 - val_accuracy: 0.6667\n",
      "Epoch 189/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7608 - accuracy: 0.7250 - val_loss: 0.8280 - val_accuracy: 0.6667\n",
      "Epoch 190/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7590 - accuracy: 0.7250 - val_loss: 0.8258 - val_accuracy: 0.6667\n",
      "Epoch 191/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7571 - accuracy: 0.7250 - val_loss: 0.8237 - val_accuracy: 0.6667\n",
      "Epoch 192/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7552 - accuracy: 0.7250 - val_loss: 0.8219 - val_accuracy: 0.6667\n",
      "Epoch 193/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7534 - accuracy: 0.7250 - val_loss: 0.8199 - val_accuracy: 0.6667\n",
      "Epoch 194/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7515 - accuracy: 0.7250 - val_loss: 0.8180 - val_accuracy: 0.6667\n",
      "Epoch 195/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7497 - accuracy: 0.7250 - val_loss: 0.8160 - val_accuracy: 0.6667\n",
      "Epoch 196/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7478 - accuracy: 0.7250 - val_loss: 0.8141 - val_accuracy: 0.6667\n",
      "Epoch 197/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7460 - accuracy: 0.7250 - val_loss: 0.8123 - val_accuracy: 0.6667\n",
      "Epoch 198/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7441 - accuracy: 0.7250 - val_loss: 0.8104 - val_accuracy: 0.6667\n",
      "Epoch 199/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.7423 - accuracy: 0.7250 - val_loss: 0.8085 - val_accuracy: 0.6667\n",
      "Epoch 200/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7405 - accuracy: 0.7250 - val_loss: 0.8065 - val_accuracy: 0.6667\n",
      "Epoch 201/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7387 - accuracy: 0.7250 - val_loss: 0.8045 - val_accuracy: 0.6667\n",
      "Epoch 202/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7368 - accuracy: 0.7250 - val_loss: 0.8027 - val_accuracy: 0.6667\n",
      "Epoch 203/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7350 - accuracy: 0.7250 - val_loss: 0.8008 - val_accuracy: 0.6667\n",
      "Epoch 204/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7332 - accuracy: 0.7250 - val_loss: 0.7990 - val_accuracy: 0.6667\n",
      "Epoch 205/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7314 - accuracy: 0.7250 - val_loss: 0.7972 - val_accuracy: 0.6667\n",
      "Epoch 206/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7296 - accuracy: 0.7250 - val_loss: 0.7952 - val_accuracy: 0.6667\n",
      "Epoch 207/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7278 - accuracy: 0.7250 - val_loss: 0.7934 - val_accuracy: 0.6667\n",
      "Epoch 208/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7260 - accuracy: 0.7333 - val_loss: 0.7916 - val_accuracy: 0.6667\n",
      "Epoch 209/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7242 - accuracy: 0.7333 - val_loss: 0.7899 - val_accuracy: 0.6667\n",
      "Epoch 210/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7225 - accuracy: 0.7333 - val_loss: 0.7881 - val_accuracy: 0.6667\n",
      "Epoch 211/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7207 - accuracy: 0.7333 - val_loss: 0.7863 - val_accuracy: 0.6667\n",
      "Epoch 212/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7190 - accuracy: 0.7333 - val_loss: 0.7846 - val_accuracy: 0.6667\n",
      "Epoch 213/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7173 - accuracy: 0.7333 - val_loss: 0.7825 - val_accuracy: 0.6667\n",
      "Epoch 214/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7154 - accuracy: 0.7333 - val_loss: 0.7808 - val_accuracy: 0.6667\n",
      "Epoch 215/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7137 - accuracy: 0.7333 - val_loss: 0.7790 - val_accuracy: 0.6667\n",
      "Epoch 216/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7120 - accuracy: 0.7333 - val_loss: 0.7771 - val_accuracy: 0.6667\n",
      "Epoch 217/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7103 - accuracy: 0.7333 - val_loss: 0.7752 - val_accuracy: 0.6667\n",
      "Epoch 218/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7086 - accuracy: 0.7333 - val_loss: 0.7733 - val_accuracy: 0.6667\n",
      "Epoch 219/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.7068 - accuracy: 0.7333 - val_loss: 0.7714 - val_accuracy: 0.6667\n",
      "Epoch 220/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7051 - accuracy: 0.7333 - val_loss: 0.7696 - val_accuracy: 0.6667\n",
      "Epoch 221/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7034 - accuracy: 0.7333 - val_loss: 0.7678 - val_accuracy: 0.6667\n",
      "Epoch 222/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7017 - accuracy: 0.7417 - val_loss: 0.7658 - val_accuracy: 0.6667\n",
      "Epoch 223/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.7000 - accuracy: 0.7417 - val_loss: 0.7641 - val_accuracy: 0.6667\n",
      "Epoch 224/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6983 - accuracy: 0.7417 - val_loss: 0.7624 - val_accuracy: 0.6667\n",
      "Epoch 225/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6966 - accuracy: 0.7417 - val_loss: 0.7607 - val_accuracy: 0.6667\n",
      "Epoch 226/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6949 - accuracy: 0.7417 - val_loss: 0.7590 - val_accuracy: 0.6667\n",
      "Epoch 227/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6932 - accuracy: 0.7417 - val_loss: 0.7573 - val_accuracy: 0.6667\n",
      "Epoch 228/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6915 - accuracy: 0.7417 - val_loss: 0.7555 - val_accuracy: 0.6667\n",
      "Epoch 229/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6899 - accuracy: 0.7417 - val_loss: 0.7539 - val_accuracy: 0.6667\n",
      "Epoch 230/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6882 - accuracy: 0.7417 - val_loss: 0.7521 - val_accuracy: 0.6667\n",
      "Epoch 231/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6865 - accuracy: 0.7417 - val_loss: 0.7502 - val_accuracy: 0.6667\n",
      "Epoch 232/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6849 - accuracy: 0.7417 - val_loss: 0.7483 - val_accuracy: 0.6667\n",
      "Epoch 233/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6832 - accuracy: 0.7417 - val_loss: 0.7465 - val_accuracy: 0.6667\n",
      "Epoch 234/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6817 - accuracy: 0.7417 - val_loss: 0.7445 - val_accuracy: 0.6667\n",
      "Epoch 235/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6801 - accuracy: 0.7417 - val_loss: 0.7425 - val_accuracy: 0.6667\n",
      "Epoch 236/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6784 - accuracy: 0.7500 - val_loss: 0.7408 - val_accuracy: 0.6667\n",
      "Epoch 237/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6768 - accuracy: 0.7500 - val_loss: 0.7390 - val_accuracy: 0.6667\n",
      "Epoch 238/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6752 - accuracy: 0.7500 - val_loss: 0.7372 - val_accuracy: 0.6667\n",
      "Epoch 239/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.6736 - accuracy: 0.7500 - val_loss: 0.7354 - val_accuracy: 0.6667\n",
      "Epoch 240/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6720 - accuracy: 0.7500 - val_loss: 0.7336 - val_accuracy: 0.6667\n",
      "Epoch 241/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6706 - accuracy: 0.7500 - val_loss: 0.7317 - val_accuracy: 0.6667\n",
      "Epoch 242/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6689 - accuracy: 0.7500 - val_loss: 0.7300 - val_accuracy: 0.6667\n",
      "Epoch 243/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6673 - accuracy: 0.7500 - val_loss: 0.7284 - val_accuracy: 0.7000\n",
      "Epoch 244/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6658 - accuracy: 0.7500 - val_loss: 0.7267 - val_accuracy: 0.7000\n",
      "Epoch 245/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.6642 - accuracy: 0.7500 - val_loss: 0.7249 - val_accuracy: 0.7000\n",
      "Epoch 246/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6626 - accuracy: 0.7500 - val_loss: 0.7232 - val_accuracy: 0.7000\n",
      "Epoch 247/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6611 - accuracy: 0.7500 - val_loss: 0.7216 - val_accuracy: 0.7000\n",
      "Epoch 248/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6595 - accuracy: 0.7500 - val_loss: 0.7199 - val_accuracy: 0.7000\n",
      "Epoch 249/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6580 - accuracy: 0.7583 - val_loss: 0.7183 - val_accuracy: 0.7000\n",
      "Epoch 250/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6565 - accuracy: 0.7583 - val_loss: 0.7166 - val_accuracy: 0.7000\n",
      "Epoch 251/300\n",
      "120/120 [==============================] - 0s 141us/sample - loss: 0.6550 - accuracy: 0.7583 - val_loss: 0.7149 - val_accuracy: 0.7000\n",
      "Epoch 252/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.6534 - accuracy: 0.7583 - val_loss: 0.7132 - val_accuracy: 0.7000\n",
      "Epoch 253/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6520 - accuracy: 0.7583 - val_loss: 0.7115 - val_accuracy: 0.7000\n",
      "Epoch 254/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6505 - accuracy: 0.7583 - val_loss: 0.7097 - val_accuracy: 0.7000\n",
      "Epoch 255/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6489 - accuracy: 0.7583 - val_loss: 0.7083 - val_accuracy: 0.7000\n",
      "Epoch 256/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6475 - accuracy: 0.7583 - val_loss: 0.7068 - val_accuracy: 0.7000\n",
      "Epoch 257/300\n",
      "120/120 [==============================] - 0s 133us/sample - loss: 0.6460 - accuracy: 0.7583 - val_loss: 0.7052 - val_accuracy: 0.7000\n",
      "Epoch 258/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6445 - accuracy: 0.7583 - val_loss: 0.7037 - val_accuracy: 0.7000\n",
      "Epoch 259/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6430 - accuracy: 0.7583 - val_loss: 0.7021 - val_accuracy: 0.7000\n",
      "Epoch 260/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6416 - accuracy: 0.7583 - val_loss: 0.7005 - val_accuracy: 0.7000\n",
      "Epoch 261/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6402 - accuracy: 0.7583 - val_loss: 0.6989 - val_accuracy: 0.7000\n",
      "Epoch 262/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6387 - accuracy: 0.7667 - val_loss: 0.6974 - val_accuracy: 0.7000\n",
      "Epoch 263/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6373 - accuracy: 0.7667 - val_loss: 0.6959 - val_accuracy: 0.7000\n",
      "Epoch 264/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6358 - accuracy: 0.7667 - val_loss: 0.6944 - val_accuracy: 0.7000\n",
      "Epoch 265/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6344 - accuracy: 0.7667 - val_loss: 0.6928 - val_accuracy: 0.7000\n",
      "Epoch 266/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6330 - accuracy: 0.7667 - val_loss: 0.6912 - val_accuracy: 0.7000\n",
      "Epoch 267/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6316 - accuracy: 0.7667 - val_loss: 0.6896 - val_accuracy: 0.7000\n",
      "Epoch 268/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6301 - accuracy: 0.7667 - val_loss: 0.6880 - val_accuracy: 0.7000\n",
      "Epoch 269/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6287 - accuracy: 0.7667 - val_loss: 0.6866 - val_accuracy: 0.7000\n",
      "Epoch 270/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6273 - accuracy: 0.7667 - val_loss: 0.6850 - val_accuracy: 0.7000\n",
      "Epoch 271/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6259 - accuracy: 0.7667 - val_loss: 0.6834 - val_accuracy: 0.7000\n",
      "Epoch 272/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6245 - accuracy: 0.7750 - val_loss: 0.6818 - val_accuracy: 0.7000\n",
      "Epoch 273/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.6232 - accuracy: 0.7750 - val_loss: 0.6802 - val_accuracy: 0.7000\n",
      "Epoch 274/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6217 - accuracy: 0.7750 - val_loss: 0.6787 - val_accuracy: 0.7000\n",
      "Epoch 275/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6204 - accuracy: 0.7750 - val_loss: 0.6771 - val_accuracy: 0.7000\n",
      "Epoch 276/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6190 - accuracy: 0.7750 - val_loss: 0.6756 - val_accuracy: 0.7000\n",
      "Epoch 277/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6177 - accuracy: 0.7750 - val_loss: 0.6742 - val_accuracy: 0.7000\n",
      "Epoch 278/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6163 - accuracy: 0.7750 - val_loss: 0.6726 - val_accuracy: 0.7000\n",
      "Epoch 279/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6149 - accuracy: 0.7833 - val_loss: 0.6711 - val_accuracy: 0.7000\n",
      "Epoch 280/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6136 - accuracy: 0.7833 - val_loss: 0.6698 - val_accuracy: 0.7000\n",
      "Epoch 281/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6122 - accuracy: 0.7833 - val_loss: 0.6682 - val_accuracy: 0.7000\n",
      "Epoch 282/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6109 - accuracy: 0.7833 - val_loss: 0.6667 - val_accuracy: 0.7000\n",
      "Epoch 283/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6095 - accuracy: 0.7833 - val_loss: 0.6652 - val_accuracy: 0.7000\n",
      "Epoch 284/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6083 - accuracy: 0.7833 - val_loss: 0.6636 - val_accuracy: 0.7000\n",
      "Epoch 285/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6069 - accuracy: 0.7833 - val_loss: 0.6622 - val_accuracy: 0.7000\n",
      "Epoch 286/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.6055 - accuracy: 0.7833 - val_loss: 0.6606 - val_accuracy: 0.7333\n",
      "Epoch 287/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6042 - accuracy: 0.7833 - val_loss: 0.6591 - val_accuracy: 0.7333\n",
      "Epoch 288/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.6029 - accuracy: 0.7917 - val_loss: 0.6576 - val_accuracy: 0.7333\n",
      "Epoch 289/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.6016 - accuracy: 0.7917 - val_loss: 0.6561 - val_accuracy: 0.7667\n",
      "Epoch 290/300\n",
      "120/120 [==============================] - 0s 183us/sample - loss: 0.6003 - accuracy: 0.7917 - val_loss: 0.6546 - val_accuracy: 0.7667\n",
      "Epoch 291/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5990 - accuracy: 0.7917 - val_loss: 0.6531 - val_accuracy: 0.7667\n",
      "Epoch 292/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5977 - accuracy: 0.7917 - val_loss: 0.6517 - val_accuracy: 0.7667\n",
      "Epoch 293/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5964 - accuracy: 0.7917 - val_loss: 0.6504 - val_accuracy: 0.7667\n",
      "Epoch 294/300\n",
      "120/120 [==============================] - 0s 125us/sample - loss: 0.5951 - accuracy: 0.7917 - val_loss: 0.6489 - val_accuracy: 0.7667\n",
      "Epoch 295/300\n",
      "120/120 [==============================] - 0s 100us/sample - loss: 0.5938 - accuracy: 0.8000 - val_loss: 0.6474 - val_accuracy: 0.7667\n",
      "Epoch 296/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.5926 - accuracy: 0.8000 - val_loss: 0.6460 - val_accuracy: 0.8000\n",
      "Epoch 297/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5913 - accuracy: 0.8083 - val_loss: 0.6444 - val_accuracy: 0.8000\n",
      "Epoch 298/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.5900 - accuracy: 0.8083 - val_loss: 0.6430 - val_accuracy: 0.8000\n",
      "Epoch 299/300\n",
      "120/120 [==============================] - 0s 108us/sample - loss: 0.5888 - accuracy: 0.8083 - val_loss: 0.6416 - val_accuracy: 0.8000\n",
      "Epoch 300/300\n",
      "120/120 [==============================] - 0s 116us/sample - loss: 0.5875 - accuracy: 0.8167 - val_loss: 0.6401 - val_accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28b262d8fc8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaled_X_train, \n",
    "          y=y_train, \n",
    "          epochs=300,\n",
    "          validation_data = (scaled_X_test, y_test),\n",
    "         callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.318301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.358546</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.309765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.350232</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.300686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.341859</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.292231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333643</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.283776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.325593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.592562</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.645972</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.591341</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.644417</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.590016</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.642986</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.588780</td>\n",
       "      <td>0.808333</td>\n",
       "      <td>0.641560</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.587542</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.640119</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss  accuracy  val_loss  val_accuracy\n",
       "0    1.318301  0.000000  1.358546           0.0\n",
       "1    1.309765  0.000000  1.350232           0.0\n",
       "2    1.300686  0.000000  1.341859           0.0\n",
       "3    1.292231  0.000000  1.333643           0.0\n",
       "4    1.283776  0.000000  1.325593           0.0\n",
       "..        ...       ...       ...           ...\n",
       "295  0.592562  0.800000  0.645972           0.8\n",
       "296  0.591341  0.808333  0.644417           0.8\n",
       "297  0.590016  0.808333  0.642986           0.8\n",
       "298  0.588780  0.808333  0.641560           0.8\n",
       "299  0.587542  0.816667  0.640119           0.8\n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics[['val_accuracy','accuracy']]\n",
    "losses = metrics[['loss','val_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28c6ef50d48>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x28c6ef137c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deVwV9f7H8deXXcWdXQRRERRwBfd9STNLK0tb1Sxv2WalLbe72Hbb69Yv08zULHMpW620MvdcQAQVd0AUUMEFFNkP398fc7yRAqIczuHA5/l4+JAzM8x8prE3w3e+8/0qrTVCCCHsn4OtCxBCCGEZEuhCCFFLSKALIUQtIYEuhBC1hAS6EELUEk62OrCHh4du1aqVrQ4vhBB2aceOHae01p5lrbNZoLdq1YqYmBhbHV4IIeySUiqlvHXS5CKEELWEBLoQQtQSEuhCCFFL2KwNXQhRNxUVFZGamkp+fr6tS6nR3Nzc8Pf3x9nZudLfI4EuhLCq1NRUGjZsSKtWrVBK2bqcGklrzenTp0lNTSUoKKjS3ydNLkIIq8rPz6d58+YS5hVQStG8efOr/i1GAl0IYXUS5ld2Lf+N7C/QTyfCz8+CqcjWlQghRI1in4G+bTbsWm7rSoQQdsrd3d3WJVQL+wv04GHg0xE2vg0lJltXI4QQNYb9BbpS0H8GnEmEhG9sXY0Qwo5prZkxYwbh4eFERESwbNkyAI4fP07//v3p3Lkz4eHhbNy4EZPJxMSJE/+37bvvvmvj6i9nn90WQ0eBZ6hxlx52CzjY388lIQS88EMCe9PPWXSfHfwa8e8bwyq17ddff01cXBzx8fGcOnWKqKgo+vfvzxdffMHw4cN5/vnnMZlM5ObmEhcXR1paGnv27AEgKyvLonVbgn0moYMD9JsOGXvhwE+2rkYIYac2bdrEHXfcgaOjI97e3gwYMIDo6GiioqJYsGABM2fOZPfu3TRs2JDWrVuTlJTEo48+yqpVq2jUqJGty7+Mfd6hA4TdDOv+AxvehNAbjKYYIYRdqeyddHXRWpe5vH///mzYsIEff/yRe+65hxkzZnDvvfcSHx/P6tWrmTVrFsuXL2f+/PlWrrhi9nmHDuDoBH2fhONxcPg3W1cjhLBD/fv3Z9myZZhMJjIzM9mwYQPdu3cnJSUFLy8vHnjgASZPnkxsbCynTp2ipKSEW2+9lZdeeonY2Fhbl38Z+71DB+g4Dta/DuvfgLZD5S5dCHFVbr75ZrZs2UKnTp1QSvHGG2/g4+PDp59+yptvvomzszPu7u4sWrSItLQ0Jk2aRElJCQCvvvqqjau/nCrvV47qFhkZqS0ywcX2j+Gn6TDhBwjqX/X9CSGq1b59+2jfvr2ty7ALZf23Ukrt0FpHlrW93TW5lJRotiWd/nNBl3vA3dtoSxdCiDrM7gJ9ecwxxs3dSuzRs8YCZzfo/Rgkb4CULbYtTgghbMjuAv3GTn40rufMnHWJfy6MvA/cfeC3mWCjJiQhhLC1Kwa6Umq+UipDKbWnnPWjlVK7lFJxSqkYpVRfy5f5pwauTkzo3Ypf9p7kcMZ5Y6FLfRj4DBzbCgd+rs7DCyFEjVWZO/SFwIgK1q8BOmmtOwP3AfMsUFeFJvZuhZuzAx+tT/pzYZd7oHlbWPOCjPEihKiTrhjoWusNwJkK1ufoP7vKNACqvc2jWQMXxkcF8G1cGulZecZCR2cY/E/I3A/xS6q7BCGEqHEs0oaulLpZKbUf+BHjLr287aaYm2ViMjMzq3TM+/sFUaLhk03Jfy7sMBpadIO1/4GivCrtXwgh7I1FAl1r/Y3WOhQYA7xUwXZztdaRWutIT0/PKh3Tv2l9burkx5LtR8nKLTQWKgVDX4BzabD1wyrtXwghoOKx048cOUJ4eLgVq6mYRXu5mJtn2iilPCy53/L8bUBrcgtNLNqS8ufCoH4QMhI2vgPnT1qjDCGEqBGq/Oq/UqotkKi11kqproALcPoK32YRoT6NGBLqxYLNydzfL4j6LubTue5lmNUDfn8RRs+yRilCiGvx87NwYrdl9+kTAde/Vu7qZ555hsDAQKZOnQrAzJkzUUqxYcMGzp49S1FRES+//DKjR4++qsPm5+fz0EMPERMTg5OTE++88w6DBg0iISGBSZMmUVhYSElJCStWrMDPz4/bb7+d1NRUTCYT//znPxk3blyVThsq121xCbAFCFFKpSqlJiulHlRKPWje5FZgj1IqDpgFjNNWHE/gwYFtOJtbxPLoY38ubN4GevwNdi6G9J3WKkUIYQfGjx//v4ksAJYvX86kSZP45ptviI2NZe3atTz11FPljsRYnlmzjJvH3bt3s2TJEiZMmEB+fj5z5szh8ccfJy4ujpiYGPz9/Vm1ahV+fn7Ex8ezZ88eRoyoqCNh5V3xDl1rfccV1r8OvG6Raq5BVKtmRAY25eONydzVMxBnR/PPqP4zjHlHf5gG968xRmcUQtQsFdxJV5cuXbqQkZFBeno6mZmZNG3aFF9fX5544gk2bNiAg4MDaWlpnDx5Eh8fn0rvd9OmTTz66KMAhIaGEhgYyMGDB+nVqxevvPIKqamp3HLLLQQHBxMREcH06dN55plnGDVqFP369bPIudndm6JleWhgG9Ky8vgmNu3PhfWaGP9YjsfB9rm2K04IUeOMHTuWr776imXLljF+/HgWL15MZmYmO3bsIC4uDm9vb/Lz869qn+Xd0d955518//331KtXj+HDh/P777/Trl07duzYQUREBM899xwvvviiJU6rdgT64FAvOrVswn9/O0h+UamXisJugbbD4PeXIetY+TsQQtQp48ePZ+nSpXz11VeMHTuW7OxsvLy8cHZ2Zu3ataSkpFx5J5fo378/ixcvBuDgwYMcPXqUkJAQkpKSaN26NY899hg33XQTu3btIj09nfr163P33Xczffp0i42tXisCXSnFM8NDSM/O5/OtKaVXwA1vAxp+miHjvAghAAgLC+P8+fO0aNECX19f7rrrLmJiYoiMjGTx4sWEhoZe9T6nTp2KyWQiIiKCcePGsXDhQlxdXVm2bBnh4eF07tyZ/fv3c++997J79266d+9O586deeWVV/jHP/5hkfOy//HQS7l73jb2Hj/HhqcH4e5aqs188/vw6z/h9kXGy0dCCJuR8dArr9aPh16RGcNDOHOhkHkbk/66oudUoyvTT09DfrZtihNCiGpWqwK9U8smXB/uw8cbkjidU/DnCkcnuPE9uJABq5+3XYFCCLu0e/duOnfu/Jc/PXr0sHVZl6l1ffmeui6EX/ae5O1fD/KfmyP+XNGiG/R5HDa9a7xJGjrSdkUKUcdprVF2NAdwREQEcXFxVj3mtTSH16o7dIC2Xu5M6NWKJduPsjv1kuaVgX8H7wj44TG4cMo2BQpRx7m5uXH69OlrCqy6QmvN6dOncXNzu6rvq1UPRS86l1/E4LfWEdCsPl892BsHh1J3Aif3wtwBEHwdjPvc6AkjhLCaoqIiUlNTr7qfd13j5uaGv78/zs7Of1le0UPRWtfkAtDIzZlnRoQy46tdfL0zjbHd/P9c6d3BGDf9139C3BfQ5S7bFSpEHeTs7ExQUJCty6iVal2Ty0W3dvWnS0ATXvt5P+fyi/66stfDENgHfn4Gzl79CwRCCFET1dpAd3BQvHBTGKcvFPD+b4cuWekIY2YbX6+YDMWF1i9QCCEsrNYGOkBH/yaMj2rJgj+OcPDk+b+ubBoIo/8PUqPht5k2qU8IISypVgc6wPTrQmjk5sTTX+3CVHLJA+Cwm6H732DrLNj3g20KFEIIC6n1gd7c3ZWZN4URdyyLTzYlXb7BdS+BX1f49mE4U8Z6IYSwE7U+0AFu6uTHsA7evPXLQRIzc/660skVblsIClg+AYqkK5UQwj7ViUBXSvHKmHDqOTuW3fTSNBDGzIETu+DHp2RURiGEXarMFHTzlVIZSqk95ay/Sym1y/znD6VUJ8uXWXVejdz4940d2JFylgWbky/fIHQk9H8a4j43hgcQQgg7U5k79IVARRPeJQMDtNYdgZeAGjs90M1dWjAk1Iu3fjlA8qkLl28w6O8QfiuseQESvrV+gUIIUQVXDHSt9QbgTAXr/9BanzV/3Ar4l7etrSml+M8tEbg6OfLw4ti/zm5kbACjPwT/7vDN3yC1eoYmEEKI6mDpNvTJwM/lrVRKTVFKxSilYjIzMy186MrxbuTGu+M6sff4Of79XcLlGzi7wR1LwN0bloyXN0mFEHbDYoGulBqEEejPlLeN1nqu1jpSax3p6elpqUNftcGh3jwyqC3LYo6xPLqMuUYbeMBdXxpvkH4xTibFEELYBYsEulKqIzAPGK21Pm2JfVa3J4a1o0/b5vzzuz3sSSsjsD1DYNwiOH0Ilt0t3RmFEDVelQNdKRUAfA3co7U+WPWSrMPRQfHe+C40re/C1MWxZOcVXb5R64EwehYkb4Cv7gNTGdsIIUQNUZlui0uALUCIUipVKTVZKfWgUupB8yb/ApoDHyql4pRSdvMk0cPdlVl3dSU9K4+nlsdRcmn/dIBO4+H6N+HAj/Ddw1BSYv1ChRCiEq44HrrW+o4rrL8fuN9iFVlZt8Cm/H1ke15cuZc5GxKZOrDt5Rv1mAIF2fD7y+DSAEa+DQ514p0sIYQdqZUTXFytSX1asePoWd5afYDOLZvQu43H5Rv1mw4FObD5v6BL4IZ3JdSFEDWKJBJG//TXb+1IkEcDHluykxPZZTwAVQqGzoS+T8KOhfDDo9L8IoSoUSTQzdxdnZhzdzdyC01M+SyG3MLiyzdSCob8yxgiYOfn5jZ10+XbCSGEDUiglxLs3ZD3x3dhT1o2j3yxk2JTGXfgSsHg52Hg3yH+C/jmQTCVEf5CCGFlEuiXGNrBmxdHh/P7/gz+9X0CuryRFwc+Y0w2vXs5fDNFQl0IYXPyULQMd/cMJC0rj9nrEmnRpB4PDyqj5wtA/+ng4AS//RtKiuHWT8DR2brFCiGEmQR6OWZcF8LxrDzeXH0An0Zu3NqtnDHH+k4zQv2X54329LELwMnFusUKIQTS5FIuBwfFG2M70adtc55esYufdx8vf+Pej8CI12H/Slh+DxTmWq9QIYQwk0CvgIuTA3PviaRLyyY8tnQnv+8/Wf7GPR+EG96Bg6th0WjILXfEYSGEqBYS6FfQwNWJ+ZOiaO/biAc/j2XToVPlbxw12Zif9Hg8zB8OWUetVqcQQkigV0IjN2cW3ded1h4NuH9RNNuSKhhQMmwM3PMNnD8Jn1wHJ8qcuU8IISxOAr2SmtR34fP7e+DftD6TFkbzR2IFd+qt+sB9PwMKFlwPyRutVqcQou6SQL8KHu6uLHmgJy2b1mfSgmjWHcgof2PvMLj/V2joC5/dDPFLrVeoEKJOkkC/Sp4NXVkypSdtvdx5YFEMvyScKH/jxv4weTUE9DTmKP39ZRn/RQhRbSTQr0GzBi588UBPwvwaM3VxLCt3pZe/cb2mcPfX0OUe2PAmrJgMRXnWK1YIUWdIoF+jxvWc+fz+HnQNaMpjS3by1Y7U8jd2coGb/g+GvgAJX8PCUXCugn7tQghxDSozY9F8pVSGUqrM7hpKqVCl1BalVIFSarrlS6y53F2dWHhfFL3beDD9y3g+3pBU/sZKGW+V3v4ZZOyDj/rDkc3WK1YIUetV5g59ITCigvVngMeAtyxRkL2p7+LEJxMjuaGjL6/8tI9Xftxb9lR2F3W4CR5YA64N4dMbYcuHUN4AYEIIcRWuGOha6w0YoV3e+gytdTRQZ2dQdnVy5P/Gd2FCr0A+3pjMk8vjKCyu4OGnV3uYshbajYDVz8Hi2+B8BQ9XhRCiEqQN3UIcHBQzbwpjxvAQvo1LZ/Kn0ZzPr+BnnFtjGL8YRr4FRzbBh70g4VvrFSyEqHWsGuhKqSlKqRilVExmZqY1D20VSikeHtSWN8Z25I/E09w2ZwvpWRX0aFEKuj8AD26Epq3gywmw9C44V0GvGSGEKIdVA11rPVdrHam1jvT09LTmoa3q9siWLJwURdrZPG7+cDN70rIr/gaPYJj8i9EL5vAa+KA7bJ0DpjrbiiWEuAbS5FJN+gV78uVDvXBUits/2lLxSI1gTIzRdxpM3QIto2DVM/BBFOz6Ul5GEkJUiip3irWLGyi1BBgIeAAngX8DzgBa6zlKKR8gBmgElAA5QAet9bmK9hsZGaljYmKqWn+Nd/JcPpM/jWZv+jmeu7499/cLQilV8TdpDQdXGW+WntwDXh2g31MQdjM4OFqncCFEjaSU2qG1jixz3ZUCvbrUlUAHyC0sZvqX8fy0+wS3dGnBf26JwM25EsFcUmK8iLT+dTh1EJoGGXfxne4AJ9fqL1wIUeNUFOjS5GIF9V2cmHVnV54c1o6vd6Yxbu5WTp7Lv/I3OjhAxFiYus14IcmtMfzwOLzXCf74AApyqr94IYTdkDt0K1u15wRPLo/D3dWJufdG0rllk8p/s9aQtBY2vgNHNhrjxPR4ELpPgfrNqq9oIUSNIXfoNciIcB++ntobV2cHbp+zhS+2HaXSP1SVgjaDYeJKmPwbBPSCda/Cu+Gw+nkZH0aIOk7u0G3k7IVCHlu6k42HTnFrV39eHhNOPZdreOB5ci9sehf2rDAemHa+E3o/Bs3bWL5oIYTNyUPRGspUonl/zSHe//0QId4NmX13N4I8Glzbzs4kwx/vw87FYCqE0BuMYA/oYdmihRA2JYFew607kMG0ZXEUmzQvjwlnTJcW176z8ych+mOIngd5Z8E/Cno/CqGjpMujELWABLodSD2by7SlccSknGV0Zz9eGhNOIzfna99h4QWI+wK2zIKzyUaXx96PGk0yzvUsV7gQwqok0O1EsamED9cl8t6aQ/g0cuO/4zsT1aqKvVdKTLB/JWx+D9J2QANP6PE3iLrf6CUjhLArEuh2JvboWaYtjSP1bC5TB7bl0SFtcXWqYnOJ1saojpvfg8O/gnMD6DYRek015j4VQtgFCXQ7lFNQzMzvE/hqRyqtPRow86Yw+rez0IBmJ/YYD1B3f2V0hYy4zXiA6t3BMvsXQlQbCXQ7tu5ABjO/T+DI6VyuD/fhH6M60KKJhdrAs44aMybFfgpFuRAyEgY+B74dLbN/IYTFSaDbuYJiEx9vSOKDtYdRKB7oF8QD/VvTsCoPTUvLPQPb58LWDyE/G9rfaAS7d5hl9i+EsBgJ9Foi9Wwur/68nx93Had5AxceHdyWO3sE4uJkoRd+87KMUN/yIRSeN0Z3HPgceIZYZv9CiCqTQK9l4o9l8drP+9mSdJqAZvWZPjyEURG+ODhcYVjeyso9A1s+MCbZKMo12tgHPAMebS2zfyHENZNAr4W01qw/mMlrP+9n/4nzRLRozNMjQujb1uPK461X1oVTRq+Y7R+DqcAYtrf/DGgWZJn9CyGumgR6LVZSovkuPo23Vh8kLSuPnq2b8fSIULoGWLCPeU4GbPovxHwCJcXGy0n9Z0CTAMsdQwhRKRLodUBBsYkl247ywdrDnMopZGh7b2YMDyHEp6HlDnLuOGx6B3YsNPq1d73XmEmpcRWGKhBCXJUqBbpSaj4wCsjQWoeXsV4B7wEjgVxgotY69kpFSaBXjwsFxSzYnMxH65PIKSxmTOcWPDG0HQHN61vuINmpsPFtiP3M6MfebRL0exIa+ljuGEKIMlU10PtjzBO6qJxAHwk8ihHoPYD3tNZXHOJPAr16ZeUWMnt9Igs3H8FUormjewCPDm6LVyM3yx3kbApsfMsY4dHRGSInG1PkuXtZ7hhCiL+ocpOLUqoVsLKcQP8IWKe1XmL+fAAYqLWucLYFCXTrOHkun/fXHGJZ9DGcHBUTewfx0IA2NK5voT7sAGeSYP2bsGspOLlB9weg9+PQoLnljiGEAKp/xqIWwLFSn1PNy8oqZIpSKkYpFZOZmWmBQ4sr8W7kxis3R7DmqQEMD/Phow2JDHhrLfM3JVNYXGKZgzRrDTfPhoejjXHYN78P73WENS8ZXSCFEFZhiUAvq49cmbf9Wuu5WutIrXWkp6eFxiURlRLYvAHvje/Cj4/2I9yvMS+u3Mt1765n1Z4TlZ8C70o82sKt82DqVggeZjTHvNcJ1r5qvLQkhKhWlgj0VKBlqc/+QLoF9iuqQQe/Rnw2uTsLJkbh5OjAg5/vYNzcrexKtWDgeoXCbQvhwc3QegCsf824Y1//JuSfs9xxhBB/YYlA/x64Vxl6AtlXaj8XtqWUYlCoF6se78fLY8JJzMjhpg82M23pTtKy8ix3IJ9wGPc5/G0DBPSGtS8bwb7pXSjIsdxxhBBA5Xq5LAEGAh7ASeDfgDOA1nqOudviB8AIjG6Lk7TWV3zaKQ9Fa47z+UXMXpfIvE3JKOD+fkE8NLAt7q5Olj1Q2g6j+eXwr1DfA/o8bky04WLBLpVC1HLyYpGolLSsPN5ctZ9v49LxcHfhiWHtGBfZEidHCw3+ddGx7bD2FUhaZ8yg1PcJiLxPpsYTohIk0MVViT+WxSs/7mP7kTOEeDfkxdFh9GhdDV0QU7bAuv9A8gZw9zaCvdtECXYhKiCBLq6a1prVCSd4aeU+0rLyGNPZj7+PbG/ZF5MuOrIZ1r0KRzaCu4/x1mnXCeBcDccSws5JoItrlldoYva6w8xZn4SLkwPThgYzoXcrnC3dDAOQvNEI9pTN0NDPHOz3gpOr5Y8lhJ2SQBdVduTUBWb+kMC6A5m083bnxdHh9KyOZhitIXm98fD02FZo5A/9n4LOd4OTi+WPJ4SdkUAXFqG15te9J3lx5V5Sz+Yx2twM410dzTBaQ9JaI9hTt0PjAOg/3Ri619GCwxYIYWck0IVF5ReZ+HBdInPWJ+LsoJg2tB0T+1RTM4zWcHiN8fA0bQc0CYQBT0PH8eBo4W6VQtgBCXRRLVJOX2Dm9wmsPZBJsJfRDNOrTTUNyKU1HPrF6O54PN4YP2bAMxA+VoJd1CkS6KLaaK35bV8GL/yQQOrZPG7q5MfzN1RTM4xxQDjws3HHfmI3NG8LA56F8FvAwbF6jilEDSKBLqrdpc0wjw8NZlKfoOpphgEoKYEDPxpt7BkJ4BECA5+BDjeDQzUdU4gaQAJdWE3K6Qu88MNeft+fQbCXOy+MDqN3G4/qO2BJCez7Dta9Bpn7wbM9DHwW2t8kwS5qJQl0YXW/7T3JCysTOHbGCs0wACUmSPjGCPbTh8A73Aj20FHGNHlC1BIS6MIm8otMzF6XyGxr9Ia5qMQEe1YYwX4mEXw6wqC/Q7sREuyiVpBAFzZ1aTPMv28Mo29wNTbDAJiKYfeXxljsZ4+AXxcY+Hdj4g0JdmHHJNBFjWC8lGQ0w1zXwZt/3NCBgObVPHSuqQjil8KGNyDrKLSINO7Y2wyWYBd2SQJd1Bj5RSY+2ZTMrLWHKS7RPNAviIcHtaW+SzX3JS8uhPgvYMNbkH0MWvaEQc9B0AAJdmFXJNBFjXMiO5/XV+3nm51p+DZ24+8j2zOqoy+qusO1uAB2fgYb3obz6RDYx7hjb9W3eo8rhIVIoIsaa0fKGf71XQIJ6efo2boZM28KI9SnUfUfuCgfYhfBxrch5wQE9Tfa2AN7Vf+xhaiCKge6UmoE8B7gCMzTWr92yfpAYD7gCZwB7tZap1a0Twl0cZGpRLM0+ihvrj7A+fxi7uoRwBND29G0gRVGVyzKg5gFxjynFzKg9SDjjr1l9+o/thDXoEqBrpRyBA4Cw4BUIBq4Q2u9t9Q2XwIrtdafKqUGY8wrek9F+5VAF5fKyi3knV8PsnjbUdxdnXhiaDB39Qys3m6OFxXmQswnsOm/kHsK2g4z2thbdKv+YwtxFaoa6L2AmVrr4ebPzwForV8ttU0CMFxrnWqeNDpba13h780S6KI8B0+e56WVe9l46BRtvdz5xw3tGRjiZZ2DF+RA9Mew+T3IO2v0Xx/4HPh1ts7xhbiCigK9Mrc+LYBjpT6nmpeVFg/cav76ZqChUuqyYfeUUlOUUjFKqZjMzMxKHFrURe28G7Lovu7MuzeSYlMJExdEc9/CaBIzc6r/4K7uxtym03bD4H/C0a0wdwAsvcsYDEyIGqwygV5Wt4NLb+unAwOUUjuBAUAaUHzZN2k9V2sdqbWO9PT0vOpiRd2hlGJoB29+eWIAz49sT3TyGYa/u4GXVu4lO6+o+gtwbWhMqDFtl/GwNHkjzOkLy++Fk3uv/P1C2IBFmlwu2d4d2K+19q9ov9LkIq7GqZwC3v7lAEujj9G0vgtPDmvH+KiWOFmjfR0gLwu2fghbPoTCHAi72RgrxjPEOscXwqyqbehOGA9Fh2DceUcDd2qtE0pt4wGc0VqXKKVeAUxa639VtF8JdHEtEtKzefGHvWxLPkOoT0P+NaoDvdtW8zACpeWegS0fwNY5UJQLEbcZE214tLVeDaJOq1Ibuta6GHgEWA3sA5ZrrROUUi8qpW4ybzYQOKCUOgh4A69YpHIhLhHm15ilU3oy+66u5BQUc+e8bUxZFEPK6QvWKaB+MxjyL6ONvc9jsH8lzIqCbx6CM0nWqUGIcsiLRcJu/WUYAZNmUt9WPDKoLQ3drDiJdE6G0SMmep4xbkznO6D/DGjayno1iDpF3hQVtdrJc/m8seoAK2JTaVrfmYcHteXunoG4OVtxSrrzJ4w+7DHzQZugy93Qbzo0aWm9GkSdIIEu6oTdqdm8sXo/Gw+dwq+xG9OGteOWLi2s9+AU4Fw6bHwHYj815j/tei/0ewoaX9rTV4hrI4Eu6pQ/Dp/i9dUHiD+WRVsvd6ZfF8LwMO/qH/irtOxUY5yY2M+M0Ry7TTL6tzfytV4NolaSQBd1jtaa1QkneXP1fhIzL9CpZROeGRFSvfObluVsCmx8C3YuBkdniLwP+kyDht7WrUPUGhLoos4qNpXw9c40/vvrQdKz8+kX7MHTw0OJ8G9s3ULOJBtjsccvMYK920To8zg08rNuHcLuSaCLOi+/yMTnW1OYtfYwZ3OLuKGjL08Na0drT3frFnI6ETa9Y8yipByMNva+T0DjCt/DE+J/JNCFMDufX8THG5OZtzGJguISbo9syeNDgvFp7GbdQs4eMR6exi0GlNErpu8T0DTQunUIuyOBLsQlTuUU8MHvhyLHgGQAABYZSURBVFm8LQUHpZjYpxUPDWhDk/pWGIO9tKyjxljssZ8BGjqON4Jd3jwV5ZBAF6Icx87k8u5vB/lmZxrurk48OKANk/q0qv45Ti+VnWq8oBS7yJgmL2yM0d3RJ8K6dYgaTwJdiCs4cOI8b64+wG/7TuLZ0JXHhgQzPqqldSbXKC0nA7bMguhPoPC8MR57v+nQMsq6dYgaSwJdiErakXKG138+wPYjZwhsXp8nh7Xjxo5+ODhYsQ87GJNrbP/YGOEx7yy0HgiDnpep8YQEuhBXQ2vNuoOZvLHqAPuOn6O9byOeHhHCwHae1n05CYwZlGLmG80xMjWeQAJdiGtSUqL5YVc6b/9ykKNncuke1IxnRoTQLbCZ9Yu5bGq8641g9+1k/VqETUmgC1EFhcUlLIs5xvtrDpF5voCh7b2ZMTyEEJ+G1i8m/xxs/wj++D/Iz4bQUcZEG/LwtM6QQBfCAnILi1mw+Qhz1ieSU1DMyHBfHh3SllCfCudDrx55WbB1ttHGXnAOgq8zesUE9LR+LcKqJNCFsKCs3ELmbUxm4R9HyCko5vpwHx4bEkx7X1sE+1nYPs/88PQMBPaBfk9CmyHGoGCi1qlyoCulRgDvAY7APK31a5esDwA+BZqYt3lWa/1TRfuUQBf2Liu3kPmbklmw+QjnC4q5roM3jw0JJryFlceJASi8YPRh3/w+nE8Hn47GHXv7G8HBiuPCi2pX1TlFHTHmFB0GpGLMKXqH1npvqW3mAju11rOVUh2An7TWrSrarwS6qC2y84pYsDmZ+ZuSOZdfzND23jw+JNj6A4ABFBfCrmXG26dnEqF5MPSdBhG3g5OV34IV1aJKc4oC3YHDWuskrXUhsBQYfck2Grj4+2ZjIP1aixXC3jSu58y0oe3Y9OxgnhzWjugjZ7jxg03ctzCauGNZ1i3GyQW63gOPRMPYBeDkBt89DO93gW0fQWGudesRVlWZO/SxwAit9f3mz/cAPbTWj5Taxhf4BWgKNACGaq13lLGvKcAUgICAgG4pKSmWOg8haozz+UUs2pLCxxuTyMotYkA7Tx4fGkzXgKbWL0ZrOPybMdnG0S1Q3wN6PgRR90O9JtavR1RZVZtcbgOGXxLo3bXWj5ba5knzvt5WSvUCPgHCtdYl5e1XmlxEbZdTUMyiLUf4eEMSZ3OL6BfswbShwbbpxw6Q8ocxwuPhX8G1EURNhh4PyWQbdqaqgd4LmKm1Hm7+/ByA1vrVUtskYNzFHzN/TgJ6aq0zytuvBLqoKy4UFPP51hTmbkji9IVC+rRtzuND2tE9yEbBfjzeCPa93xmTbXS6A3o/JiM82omqBroTxkPRIUAaxkPRO7XWCaW2+RlYprVeqJRqD6wBWugKdi6BLuqa3MJiFm89ykcbEjmVU0iv1s15fGgwPVs3t01BpxNhywfG9HimQmg/ypgez7/MrBA1hCW6LY4E/ovRJXG+1voVpdSLQIzW+ntzz5aPAXeMB6RPa61/qWifEuiirsorNPHF9qPMWZ9I5vkCugc1Y9qQYHq1aW79sWLAGOFx20fG0AL52UZf9j6PGy8rSV/2GkdeLBKiBsovMrHEHOwnzxUQ1aopDw9qywBbDAIGUHDemGhjyyw4lwpeHYymmIixRtOMqBEk0IWowfKLTCyPOcbsdYkcz86ng28jHhrYhpERvjhae9heAFMR7FlhDASWsRca+UOvqcb8p642GL9G/IUEuhB2oLC4hG/j0pizPpGkzAsENq/PlP6tubWrP27ONnjb82KXx83vwZGN4NYYoh6AHn8Ddy/r1yMACXQh7EpJieaXvSeZve4w8anZeDZ0ZXLfIO7qEUBDNxs1faTGGMG+7wdwdIFO46DXI+AZYpt66jAJdCHskNaaLYmnmb0+kY2HTtHQzYl7egYyqU8Qng1dbVPU6URj6N74JVCcbzw47TnVmFFJHqBahQS6EHZud2o2s9cf5uc9J3BxdOD2yJZM6d+als3q26agC6eMeU+jP4YLmeDZ3miK6TgOXGxUUx0hgS5ELZGUmcPcDUmsiE2lRMOojr48OKCNbYbuBSguMB6gbp0NJ3ZBvabQdQJ0fwAa+9umplpOAl2IWuZEdj6fbErii21HuVBoYlCIJ1MHtSWqlY3ePtXaGCtm62zYvxJQEH6r0Z/dJ9w2NdVSEuhC1FJZuYV8tiWFBX8c4cyFQiIDm/LQwDYMDvWyTV92gKyjxotKOxZCYY7Rzt5nGgT2lnZ2C5BAF6KWyys0sSz6KB9vTCYtK48Q74Y8NLANozr64uRYmVGyq6OosxA9D7bOgdxT4B8FfZ8wJrh2sFFNtYAEuhB1RJGphB/i05m9LpFDGTm0aFKPCb0DGRcVQON6NuryWJQHOz+HP9437t49QoymmIjbZNKNayCBLkQdU1KiWbM/g3kbk9iWfIb6Lo7c1s2fiX2CCPJoYJuiTMWw91vY9F84uRsatTC6PHabIG+gXgUJdCHqsD1p2SzYfITv49MoLtEMDvFict8g2w0GpjUkrjGC/chGcGtihHq3SdAsyPr12BkJdCEEGefz+XzrURZvTeH0hUJCfRpyX98gburkZ5uhBQBSd8Af78G+laBLoO1QYzal4GEyuXU5JNCFEP+TX2Ti+7h05m9OZv+J83i4u3BXj0Du7hlouzdQz6XDjk8h9lM4fxwaB0DkROhyL7h72qamGkoCXQhxGa01fySeZv6mZNbsz8DF0YEbO/lxX99WhPk1tk1RpiI48JPROyZ5Azg4Q9gYiJwMAT2l2yMS6EKIK0jKzGHhH0f4MiaVvCITPVs3Y3Lf1gwO9bLNEL4AmQchZj7EfQEF2eAVBlH3GcML1OGHqBLoQohKyc4tYmn0UT794wjp2fkENq/PxN6tuC2yJe6uTrYpqvAC7P7KuGs/sQtc3KHTeOOu3buDbWqyIUtMQTcCeA9jCrp5WuvXLln/LjDI/LE+4KW1blLRPiXQhai5ik0lrEo4wfxNycQezaKhqxPjoloyoXcr2w0IpjWk7TCCfc/XYCqAgN4QNRna31Rn+rRXdZJoR4xJoocBqRiTRN+htd5bzvaPAl201vdVtF8JdCHsw86jZ5m/+Qg/7T6O1prhYT7c1zeIyMCmthteIPeM8bJSzCdw9gg08DRmVOo2EZoE2KYmK6lqoPcCZmqth5s/PwegtX61nO3/AP6ttf61ov1KoAthX9Kz8li0JYUl24+SnVdER//G3NcniJERvrg42ehV/pISSPrdGMr34CpjWfBw4669zeBa2fWxqoE+Fhihtb7f/PkeoIfW+pEytg0EtgL+WmtTGeunAFMAAgICuqWkpFztuQghbCy3sJgVsWks2JxMUuYFvBu5cm+vVtzRPYBmDWzY7JF1zBgQLPZTY4z2Rv7Q5W7ocletumuvaqDfBgy/JNC7a60fLWPbZzDC/LJ1l5I7dCHsW0mJZv2hTOZvSmbjoVO4OjlwS9cW3NcniGBvG/ZCKS40uj7GLoLE341lbQYbTTIhI+2+rb2iQK/MY+tUoGWpz/5AejnbjgcevrryhBD2yMFBMSjEi0EhXhw4cZ4Fm5NZEZvGku3H6Bfswb29Wtmm26OTi9F3PWyMMRjYzsVGe/uXE6C+h9FDpusE8Gxn3bqsoDJ36E4YD0WHAGkYD0Xv1FonXLJdCLAaCNKV6Dojd+hC1D6ncwpYsv0oi7akkHG+AL/GbtzRPYBxUS3xauRmu8JKTMbdeuwi4+69pBgCehl37R3G2NW0eZbotjgS+C9Gt8X5WutXlFIvAjFa6+/N28wE3LTWz1amKAl0IWqvIlMJa/ad5POtR9l0+BRODorrwry5u0eg7QYFuygnw5jkOnYRnD4Mro0gYqwR7n5dbFdXJcmLRUIIm0k+dYEvtqXw5Y5UsnKLaO3RgDt7BDC2mz9N6tuwPfvitHk7PjWG9S3OB5+ORrCH3wr1bTSd3xVIoAshbC6/yMRPu4+zeNtRdqScxdXJgRsifBkX1ZLuQc1se9eelwW7vzR6yJzYDY4uEHI9dL4L2gwBRxu9JVsGCXQhRI2yN/0cX2xP4bud6ZwvKCbIowHjolpyS9cWeDW0YVs7wPF4iFsCu5dD7mlo4AUdbzfCvQYMNSCBLoSokfIKjbv2ZdHH2H7kDI4OiiGhXozv3pL+wZ62mw8VjO6Ph34x2tsPrjIepPp1MYLdhk0yEuhCiBovMTOH5THHWLEjlVM5hfg0cuO2SH9uj2xpu/FjLrpwCnYth7jFcHKP0SQTegN0vhvaDLLqG6kS6EIIu2H0kMlgWfRR1h/MpERDn7bNGRcVwHUdvG03uxIYD1JP7DL6tu9eDnlnoaGv0be9813gEVztJUigCyHs0vHsPL6KSWVZzDFSz+bRpL4zYzq34JauLYho0di2D1KLC4ymmJ2L4fCvxhR6/t2NoQY6jIF6FQ44e80k0IUQdq2kxJhdaWn0UX7Ze5LC4hKCvdy5tZs/Yzq3wKexjR+knj8Bu5YZ4X7qADi6Gr1kOo4z5km14HADEuhCiFojO7eIH3cfZ0VsKjtSzuKgoE9bD27t6s/wMB/qudi4SSYtFnYthT0rjF4y9ZoZD1E73wF+Xas8jZ4EuhCiVko+dYFvYlNZEZtGWlYe7q5OjIzw4Zau/nRv1QwHW02fB8b8qIfXGOG+/ydjQg6PEKO9veM4aNzimnYrgS6EqNVKSjTbj5xhxY5Uftp9nAuFJvyb1uOWLi24pas/rTwa2LbAvCzjbdT4pcbbqT2nwogyp5S4Igl0IUSdkVdoYnXCCVbEprLp8Cm0hq4BTRjV0Y8bOvribctBwgDOJBlt7HKHLoQQlXciO59vdqbxXVwa+0+cRymIatWMGzv6cn2ELx7urrYu8apJoAsh6rzDGTms3JXOyl3HOZyRg4OCXm2aM6qjHyPCfGhqy9mWroIEuhBCmGmtOXDyPCvjj7NyVzpHTufi5KDo09aDGzv5cV2YN43cnG1dZrkk0IUQogxaaxLSz/HDrnRWxh8nLSsPF0cH+rfzYFRHP4Z28MbdteaMtAgS6EIIcUVaa+KOZbFy13F+3HWcE+fycXVyYFCIF6M6+TIk1Nu2fdzNJNCFEOIqlJRodhw9y8r4dH7cfYJTOQXUc3ZkSHsvRnX0Y2CIp83GlLHEFHQjgPcwpqCbp7V+rYxtbgdmAhqI11rfWdE+JdCFEPbAVKLZlnyalbuOs2rPCc5cKMTd1YlhHbwZ1dGXfsGeuDhZb5jfKgW6UsoRY5LoYUAqxiTRd2it95baJhhYDgzWWp9VSnlprTMq2q8EuhDC3hSZStiSeJqVu9JZtecE5/KLaeTmxPAwH0Z18qN3m+Y4V/MY7lUN9F7ATK31cPPn5wC01q+W2uYN4KDWel5li5JAF0LYs8LiEjYdzmRl/HF+2XuSnIJimtZ3ZkS4Lzd29KVH6+Y4VsPQAxUFemUe37YAjpX6nAr0uGSbduYDbcZolpmptV5VRiFTgCkAAQEBlTi0EELUTC5ODgwO9WZwqDf5RSbWH8xk5a7jfLszjSXbj+Lh7srICB9GdfQjMrCpVcaVqUygl1XFpbf1TkAwMBDwBzYqpcK11ll/+Sat5wJzwbhDv+pqhRCiBnJzdmR4mA/Dw3zIKzTx+/4MVu5KZ1n0MRZtScGzoSvDOngzPMyHXq2bV1ube2UCPRVoWeqzP5BexjZbtdZFQLJS6gBGwEdbpEohhLAT9VwcuaGjLzd09CWnoJg1+06yOuEE3+5M44ttR2no6sRjQ4J5oH9rix+7MoEeDQQrpYKANGA8cGkPlm+BO4CFSikPjCaYJEsWKoQQ9sbd1YnRnVswunML8otMbDp0itUJJ/BtUj0DhF0x0LXWxUqpR4DVGO3j87XWCUqpF4EYrfX35nXXKaX2AiZghtb6dLVULIQQdsjN2ZGhHbwZ2sG72o4hLxYJIYQdqaiXi/V6wwshhKhWEuhCCFFLSKALIUQtIYEuhBC1hAS6EELUEhLoQghRS0igCyFELWGzfuhKqUwg5Rq/3QM4ZcFybEnOpWaSc6mZ5FwgUGvtWdYKmwV6VSilYsrrWG9v5FxqJjmXmknOpWLS5CKEELWEBLoQQtQS9hroc21dgAXJudRMci41k5xLBeyyDV0IIcTl7PUOXQghxCUk0IUQopawu0BXSo1QSh1QSh1WSj1r63qullLqiFJqt1IqTikVY17WTCn1q1LqkPnvprausyxKqflKqQyl1J5Sy8qsXRneN1+nXUqprrar/HLlnMtMpVSa+drEKaVGllr3nPlcDiilhtum6ssppVoqpdYqpfYppRKUUo+bl9vddangXOzxurgppbYrpeLN5/KCeXmQUmqb+bosU0q5mJe7mj8fNq9vdU0H1lrbzR+MGZMSgdaACxAPdLB1XVd5DkcAj0uWvQE8a/76WeB1W9dZTu39ga7AnivVDowEfsaYZLwnsM3W9VfiXGYC08vYtoP535orEGT+N+ho63Mw1+YLdDV/3RA4aK7X7q5LBedij9dFAe7mr52Bbeb/3suB8eblc4CHzF9PBeaYvx4PLLuW49rbHXp34LDWOklrXQgsBUbbuCZLGA18av76U2CMDWspl9Z6A3DmksXl1T4aWKQNW4EmSilf61R6ZeWcS3lGA0u11gVa62TgMMa/RZvTWh/XWseavz4P7ANaYIfXpYJzKU9Nvi5aa51j/uhs/qOBwcBX5uWXXpeL1+srYIhSSl3tce0t0FsAx0p9TqXiC14TaeAXpdQOpdQU8zJvrfVxMP5RA142q+7qlVe7vV6rR8xNEfNLNX3ZxbmYf03vgnE3aNfX5ZJzATu8LkopR6VUHJAB/IrxG0SW1rrYvEnpev93Lub12UDzqz2mvQV6WT+x7K3fZR+tdVfgeuBhpVR/WxdUTezxWs0G2gCdgePA2+blNf5clFLuwApgmtb6XEWblrGspp+LXV4XrbVJa90Z8Mf4zaF9WZuZ/7bIudhboKcCLUt99gfSbVTLNdFap5v/zgC+wbjQJy/+2mv+O8N2FV618mq3u2ultT5p/p+wBPiYP399r9HnopRyxgjAxVrrr82L7fK6lHUu9npdLtJaZwHrMNrQmyilnMyrStf7v3Mxr29M5ZsE/8feAj0aCDY/KXbBeHjwvY1rqjSlVAOlVMOLXwPXAXswzmGCebMJwHe2qfCalFf798C95l4VPYHsi00ANdUlbck3Y1wbMM5lvLknQhAQDGy3dn1lMbezfgLs01q/U2qV3V2X8s7FTq+Lp1KqifnresBQjGcCa4Gx5s0uvS4Xr9dY4HdtfkJ6VWz9NPganh6PxHj6nQg8b+t6rrL21hhP5eOBhIv1Y7SVrQEOmf9uZutay6l/CcavvEUYdxSTy6sd41fIWebrtBuItHX9lTiXz8y17jL/D+ZbavvnzedyALje1vWXqqsvxq/mu4A485+R9nhdKjgXe7wuHYGd5pr3AP8yL2+N8UPnMPAl4Gpe7mb+fNi8vvW1HFde/RdCiFrC3ppchBBClEMCXQghagkJdCGEqCUk0IUQopaQQBdCiFpCAl0IIWoJCXQhhKgl/h+7IkuTx+d/xgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6401190161705017, 0.8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(scaled_X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = len(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 4, activation = 'relu', input_shape=[4,]))\n",
    "model.add(Dense(units = 3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/300\n",
      "150/150 [==============================] - 0s 2ms/sample - loss: 1.1081 - accuracy: 0.3200\n",
      "Epoch 2/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.1062 - accuracy: 0.3267\n",
      "Epoch 3/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.1043 - accuracy: 0.3467\n",
      "Epoch 4/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.1025 - accuracy: 0.3333\n",
      "Epoch 5/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.1006 - accuracy: 0.3333\n",
      "Epoch 6/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0988 - accuracy: 0.3400\n",
      "Epoch 7/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0971 - accuracy: 0.3333\n",
      "Epoch 8/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0954 - accuracy: 0.3333\n",
      "Epoch 9/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0937 - accuracy: 0.3333\n",
      "Epoch 10/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0921 - accuracy: 0.3400\n",
      "Epoch 11/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0906 - accuracy: 0.3400\n",
      "Epoch 12/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0890 - accuracy: 0.3467\n",
      "Epoch 13/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0874 - accuracy: 0.3400\n",
      "Epoch 14/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0858 - accuracy: 0.3400\n",
      "Epoch 15/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0842 - accuracy: 0.3467\n",
      "Epoch 16/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0827 - accuracy: 0.3467\n",
      "Epoch 17/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0812 - accuracy: 0.3467\n",
      "Epoch 18/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0795 - accuracy: 0.3467\n",
      "Epoch 19/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0779 - accuracy: 0.3467\n",
      "Epoch 20/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0763 - accuracy: 0.3400\n",
      "Epoch 21/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0746 - accuracy: 0.3467\n",
      "Epoch 22/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0729 - accuracy: 0.3467\n",
      "Epoch 23/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 1.0712 - accuracy: 0.3467\n",
      "Epoch 24/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 1.0695 - accuracy: 0.3600\n",
      "Epoch 25/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0677 - accuracy: 0.3600\n",
      "Epoch 26/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0658 - accuracy: 0.3667\n",
      "Epoch 27/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0639 - accuracy: 0.3667\n",
      "Epoch 28/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0621 - accuracy: 0.3800\n",
      "Epoch 29/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0601 - accuracy: 0.3800\n",
      "Epoch 30/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0581 - accuracy: 0.3933\n",
      "Epoch 31/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0561 - accuracy: 0.4000\n",
      "Epoch 32/300\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 1.0540 - accuracy: 0.3933\n",
      "Epoch 33/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0519 - accuracy: 0.3867\n",
      "Epoch 34/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0497 - accuracy: 0.4000\n",
      "Epoch 35/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0475 - accuracy: 0.4133\n",
      "Epoch 36/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0452 - accuracy: 0.4267\n",
      "Epoch 37/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0429 - accuracy: 0.4267\n",
      "Epoch 38/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0406 - accuracy: 0.4267\n",
      "Epoch 39/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0382 - accuracy: 0.4333\n",
      "Epoch 40/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0356 - accuracy: 0.4667\n",
      "Epoch 41/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0332 - accuracy: 0.4733\n",
      "Epoch 42/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0306 - accuracy: 0.5000\n",
      "Epoch 43/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0281 - accuracy: 0.5200\n",
      "Epoch 44/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0254 - accuracy: 0.5267\n",
      "Epoch 45/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0228 - accuracy: 0.5333\n",
      "Epoch 46/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0200 - accuracy: 0.5533\n",
      "Epoch 47/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0173 - accuracy: 0.5667\n",
      "Epoch 48/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0146 - accuracy: 0.5867\n",
      "Epoch 49/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0117 - accuracy: 0.5933\n",
      "Epoch 50/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 1.0089 - accuracy: 0.5933\n",
      "Epoch 51/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 1.0061 - accuracy: 0.6000\n",
      "Epoch 52/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0032 - accuracy: 0.6067\n",
      "Epoch 53/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0003 - accuracy: 0.6200\n",
      "Epoch 54/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9973 - accuracy: 0.6333\n",
      "Epoch 55/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9943 - accuracy: 0.6333\n",
      "Epoch 56/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9913 - accuracy: 0.6400\n",
      "Epoch 57/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9884 - accuracy: 0.6467\n",
      "Epoch 58/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9854 - accuracy: 0.6467\n",
      "Epoch 59/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9825 - accuracy: 0.6533\n",
      "Epoch 60/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9795 - accuracy: 0.6533\n",
      "Epoch 61/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9765 - accuracy: 0.6533\n",
      "Epoch 62/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9734 - accuracy: 0.6600\n",
      "Epoch 63/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9705 - accuracy: 0.6600\n",
      "Epoch 64/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9673 - accuracy: 0.6600\n",
      "Epoch 65/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9643 - accuracy: 0.6600\n",
      "Epoch 66/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9612 - accuracy: 0.6600\n",
      "Epoch 67/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9582 - accuracy: 0.6600\n",
      "Epoch 68/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9551 - accuracy: 0.6600\n",
      "Epoch 69/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9520 - accuracy: 0.6600\n",
      "Epoch 70/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9490 - accuracy: 0.6600\n",
      "Epoch 71/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9459 - accuracy: 0.6600\n",
      "Epoch 72/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9428 - accuracy: 0.6600\n",
      "Epoch 73/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9398 - accuracy: 0.6600\n",
      "Epoch 74/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.9367 - accuracy: 0.6600\n",
      "Epoch 75/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.9337 - accuracy: 0.6600\n",
      "Epoch 76/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9307 - accuracy: 0.6600\n",
      "Epoch 77/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9277 - accuracy: 0.6600\n",
      "Epoch 78/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9246 - accuracy: 0.6600\n",
      "Epoch 79/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9216 - accuracy: 0.6600\n",
      "Epoch 80/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9186 - accuracy: 0.6667\n",
      "Epoch 81/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9156 - accuracy: 0.6667\n",
      "Epoch 82/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.9127 - accuracy: 0.6667\n",
      "Epoch 83/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.9097 - accuracy: 0.6667\n",
      "Epoch 84/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9067 - accuracy: 0.6667\n",
      "Epoch 85/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.9037 - accuracy: 0.6667\n",
      "Epoch 86/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.9008 - accuracy: 0.6667\n",
      "Epoch 87/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8979 - accuracy: 0.6667\n",
      "Epoch 88/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8950 - accuracy: 0.6667\n",
      "Epoch 89/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8921 - accuracy: 0.6667\n",
      "Epoch 90/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8893 - accuracy: 0.6667\n",
      "Epoch 91/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8864 - accuracy: 0.6667\n",
      "Epoch 92/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8835 - accuracy: 0.6667\n",
      "Epoch 93/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8806 - accuracy: 0.6667\n",
      "Epoch 94/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8778 - accuracy: 0.6667\n",
      "Epoch 95/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8750 - accuracy: 0.6667\n",
      "Epoch 96/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8722 - accuracy: 0.6667\n",
      "Epoch 97/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8694 - accuracy: 0.6667\n",
      "Epoch 98/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8666 - accuracy: 0.6667\n",
      "Epoch 99/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8638 - accuracy: 0.6667\n",
      "Epoch 100/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8610 - accuracy: 0.6667\n",
      "Epoch 101/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8582 - accuracy: 0.6667\n",
      "Epoch 102/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8554 - accuracy: 0.6667\n",
      "Epoch 103/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8527 - accuracy: 0.6667\n",
      "Epoch 104/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8499 - accuracy: 0.6667\n",
      "Epoch 105/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8472 - accuracy: 0.6667\n",
      "Epoch 106/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8445 - accuracy: 0.6667\n",
      "Epoch 107/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8418 - accuracy: 0.6667\n",
      "Epoch 108/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8391 - accuracy: 0.6667\n",
      "Epoch 109/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8364 - accuracy: 0.6667\n",
      "Epoch 110/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8338 - accuracy: 0.6667\n",
      "Epoch 111/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8311 - accuracy: 0.6667\n",
      "Epoch 112/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8284 - accuracy: 0.6667\n",
      "Epoch 113/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8258 - accuracy: 0.6667\n",
      "Epoch 114/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8232 - accuracy: 0.6667\n",
      "Epoch 115/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8206 - accuracy: 0.6667\n",
      "Epoch 116/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8180 - accuracy: 0.6667\n",
      "Epoch 117/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8154 - accuracy: 0.6733\n",
      "Epoch 118/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8129 - accuracy: 0.6733\n",
      "Epoch 119/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8103 - accuracy: 0.6733\n",
      "Epoch 120/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8078 - accuracy: 0.6733\n",
      "Epoch 121/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.8053 - accuracy: 0.6733\n",
      "Epoch 122/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8027 - accuracy: 0.6733\n",
      "Epoch 123/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.8002 - accuracy: 0.6733\n",
      "Epoch 124/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7978 - accuracy: 0.6733\n",
      "Epoch 125/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7953 - accuracy: 0.6733\n",
      "Epoch 126/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7928 - accuracy: 0.6733\n",
      "Epoch 127/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7904 - accuracy: 0.6733\n",
      "Epoch 128/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7879 - accuracy: 0.6733\n",
      "Epoch 129/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7855 - accuracy: 0.6800\n",
      "Epoch 130/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7832 - accuracy: 0.6800\n",
      "Epoch 131/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7807 - accuracy: 0.6800\n",
      "Epoch 132/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7784 - accuracy: 0.6867\n",
      "Epoch 133/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7760 - accuracy: 0.6867\n",
      "Epoch 134/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7736 - accuracy: 0.6867\n",
      "Epoch 135/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7714 - accuracy: 0.6867\n",
      "Epoch 136/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7690 - accuracy: 0.6867\n",
      "Epoch 137/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7668 - accuracy: 0.6867\n",
      "Epoch 138/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7645 - accuracy: 0.6867\n",
      "Epoch 139/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7622 - accuracy: 0.6867\n",
      "Epoch 140/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7599 - accuracy: 0.6867\n",
      "Epoch 141/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7577 - accuracy: 0.6867\n",
      "Epoch 142/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7554 - accuracy: 0.6867\n",
      "Epoch 143/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7532 - accuracy: 0.7000\n",
      "Epoch 144/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7509 - accuracy: 0.7000\n",
      "Epoch 145/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7488 - accuracy: 0.7000\n",
      "Epoch 146/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7465 - accuracy: 0.7000\n",
      "Epoch 147/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7443 - accuracy: 0.7000\n",
      "Epoch 148/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7422 - accuracy: 0.7000\n",
      "Epoch 149/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7401 - accuracy: 0.7000\n",
      "Epoch 150/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7379 - accuracy: 0.7000\n",
      "Epoch 151/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7358 - accuracy: 0.7000\n",
      "Epoch 152/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7337 - accuracy: 0.7000\n",
      "Epoch 153/300\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.7316 - accuracy: 0.7000\n",
      "Epoch 154/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7295 - accuracy: 0.7000\n",
      "Epoch 155/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7274 - accuracy: 0.7000\n",
      "Epoch 156/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.7253 - accuracy: 0.7000\n",
      "Epoch 157/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7233 - accuracy: 0.7000\n",
      "Epoch 158/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7212 - accuracy: 0.7000\n",
      "Epoch 159/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7193 - accuracy: 0.7000\n",
      "Epoch 160/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7173 - accuracy: 0.7000\n",
      "Epoch 161/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7152 - accuracy: 0.7000\n",
      "Epoch 162/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7132 - accuracy: 0.7000\n",
      "Epoch 163/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7113 - accuracy: 0.7000\n",
      "Epoch 164/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7094 - accuracy: 0.7067\n",
      "Epoch 165/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7074 - accuracy: 0.7067\n",
      "Epoch 166/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7054 - accuracy: 0.7067\n",
      "Epoch 167/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.7035 - accuracy: 0.7067\n",
      "Epoch 168/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.7016 - accuracy: 0.7067\n",
      "Epoch 169/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6997 - accuracy: 0.7067\n",
      "Epoch 170/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6978 - accuracy: 0.7067\n",
      "Epoch 171/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6959 - accuracy: 0.7067\n",
      "Epoch 172/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6941 - accuracy: 0.7067\n",
      "Epoch 173/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6922 - accuracy: 0.7067\n",
      "Epoch 174/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6904 - accuracy: 0.7067\n",
      "Epoch 175/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6886 - accuracy: 0.7067\n",
      "Epoch 176/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6869 - accuracy: 0.7067\n",
      "Epoch 177/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6851 - accuracy: 0.7067\n",
      "Epoch 178/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6832 - accuracy: 0.7067\n",
      "Epoch 179/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6815 - accuracy: 0.7067\n",
      "Epoch 180/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6797 - accuracy: 0.7067\n",
      "Epoch 181/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6779 - accuracy: 0.7067\n",
      "Epoch 182/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6762 - accuracy: 0.7067\n",
      "Epoch 183/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6744 - accuracy: 0.7067\n",
      "Epoch 184/300\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.6727 - accuracy: 0.7067\n",
      "Epoch 185/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6710 - accuracy: 0.7067\n",
      "Epoch 186/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6692 - accuracy: 0.7067\n",
      "Epoch 187/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6676 - accuracy: 0.7067\n",
      "Epoch 188/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6658 - accuracy: 0.7067\n",
      "Epoch 189/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6642 - accuracy: 0.7133\n",
      "Epoch 190/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6625 - accuracy: 0.7133\n",
      "Epoch 191/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6608 - accuracy: 0.7133\n",
      "Epoch 192/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6592 - accuracy: 0.7133\n",
      "Epoch 193/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6576 - accuracy: 0.7133\n",
      "Epoch 194/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6559 - accuracy: 0.7133\n",
      "Epoch 195/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6543 - accuracy: 0.7133\n",
      "Epoch 196/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6527 - accuracy: 0.7200\n",
      "Epoch 197/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6511 - accuracy: 0.7200\n",
      "Epoch 198/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.6495 - accuracy: 0.7200\n",
      "Epoch 199/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6479 - accuracy: 0.7200\n",
      "Epoch 200/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6463 - accuracy: 0.7200\n",
      "Epoch 201/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6448 - accuracy: 0.7267\n",
      "Epoch 202/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6432 - accuracy: 0.7267\n",
      "Epoch 203/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6417 - accuracy: 0.7267\n",
      "Epoch 204/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6401 - accuracy: 0.7333\n",
      "Epoch 205/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6386 - accuracy: 0.7333\n",
      "Epoch 206/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6371 - accuracy: 0.7333\n",
      "Epoch 207/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6356 - accuracy: 0.7400\n",
      "Epoch 208/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6341 - accuracy: 0.7400\n",
      "Epoch 209/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6326 - accuracy: 0.7400\n",
      "Epoch 210/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6312 - accuracy: 0.7400\n",
      "Epoch 211/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6297 - accuracy: 0.7400\n",
      "Epoch 212/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6283 - accuracy: 0.7400\n",
      "Epoch 213/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6268 - accuracy: 0.7400\n",
      "Epoch 214/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6253 - accuracy: 0.7400\n",
      "Epoch 215/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6240 - accuracy: 0.7400\n",
      "Epoch 216/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6225 - accuracy: 0.7400\n",
      "Epoch 217/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6211 - accuracy: 0.7400\n",
      "Epoch 218/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6197 - accuracy: 0.7400\n",
      "Epoch 219/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6183 - accuracy: 0.7400\n",
      "Epoch 220/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6169 - accuracy: 0.7400\n",
      "Epoch 221/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6155 - accuracy: 0.7467\n",
      "Epoch 222/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6142 - accuracy: 0.7467\n",
      "Epoch 223/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6128 - accuracy: 0.7467\n",
      "Epoch 224/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6115 - accuracy: 0.7467\n",
      "Epoch 225/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6101 - accuracy: 0.7467\n",
      "Epoch 226/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6088 - accuracy: 0.7467\n",
      "Epoch 227/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6075 - accuracy: 0.7467\n",
      "Epoch 228/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6062 - accuracy: 0.7467\n",
      "Epoch 229/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6048 - accuracy: 0.7467\n",
      "Epoch 230/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.6035 - accuracy: 0.7467\n",
      "Epoch 231/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.6023 - accuracy: 0.7467\n",
      "Epoch 232/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6010 - accuracy: 0.7467\n",
      "Epoch 233/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5997 - accuracy: 0.7467\n",
      "Epoch 234/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5985 - accuracy: 0.7467\n",
      "Epoch 235/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5972 - accuracy: 0.7467\n",
      "Epoch 236/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5959 - accuracy: 0.7467\n",
      "Epoch 237/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5947 - accuracy: 0.7467\n",
      "Epoch 238/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5934 - accuracy: 0.7467\n",
      "Epoch 239/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5922 - accuracy: 0.7467\n",
      "Epoch 240/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5910 - accuracy: 0.7467\n",
      "Epoch 241/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5899 - accuracy: 0.7467\n",
      "Epoch 242/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5886 - accuracy: 0.7467\n",
      "Epoch 243/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5874 - accuracy: 0.7467\n",
      "Epoch 244/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5862 - accuracy: 0.7533\n",
      "Epoch 245/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5850 - accuracy: 0.7533\n",
      "Epoch 246/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5838 - accuracy: 0.7533\n",
      "Epoch 247/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5827 - accuracy: 0.7533\n",
      "Epoch 248/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5816 - accuracy: 0.7533\n",
      "Epoch 249/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5804 - accuracy: 0.7533\n",
      "Epoch 250/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5793 - accuracy: 0.7533\n",
      "Epoch 251/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5781 - accuracy: 0.7533\n",
      "Epoch 252/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5770 - accuracy: 0.7533\n",
      "Epoch 253/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5758 - accuracy: 0.7533\n",
      "Epoch 254/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5747 - accuracy: 0.7600\n",
      "Epoch 255/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5736 - accuracy: 0.7600\n",
      "Epoch 256/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5725 - accuracy: 0.7667\n",
      "Epoch 257/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5714 - accuracy: 0.7667\n",
      "Epoch 258/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5703 - accuracy: 0.7667\n",
      "Epoch 259/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5692 - accuracy: 0.7667\n",
      "Epoch 260/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5681 - accuracy: 0.7667\n",
      "Epoch 261/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5671 - accuracy: 0.7733\n",
      "Epoch 262/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5659 - accuracy: 0.7800\n",
      "Epoch 263/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5649 - accuracy: 0.7800\n",
      "Epoch 264/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5638 - accuracy: 0.7800\n",
      "Epoch 265/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5627 - accuracy: 0.7800\n",
      "Epoch 266/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5617 - accuracy: 0.7800\n",
      "Epoch 267/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5607 - accuracy: 0.7800\n",
      "Epoch 268/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5596 - accuracy: 0.7800\n",
      "Epoch 269/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5586 - accuracy: 0.7800\n",
      "Epoch 270/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5576 - accuracy: 0.7800\n",
      "Epoch 271/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5566 - accuracy: 0.7800\n",
      "Epoch 272/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5555 - accuracy: 0.7800\n",
      "Epoch 273/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5546 - accuracy: 0.7800\n",
      "Epoch 274/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5535 - accuracy: 0.7800\n",
      "Epoch 275/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.5073 - accuracy: 0.81 - 0s 66us/sample - loss: 0.5525 - accuracy: 0.7800\n",
      "Epoch 276/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5516 - accuracy: 0.7933\n",
      "Epoch 277/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5505 - accuracy: 0.8000\n",
      "Epoch 278/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5496 - accuracy: 0.8000\n",
      "Epoch 279/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5486 - accuracy: 0.8000\n",
      "Epoch 280/300\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.5476 - accuracy: 0.8000\n",
      "Epoch 281/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5466 - accuracy: 0.8000\n",
      "Epoch 282/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5458 - accuracy: 0.8000\n",
      "Epoch 283/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5447 - accuracy: 0.8067\n",
      "Epoch 284/300\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5438 - accuracy: 0.8067\n",
      "Epoch 285/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5428 - accuracy: 0.8067\n",
      "Epoch 286/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5419 - accuracy: 0.8067\n",
      "Epoch 287/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5410 - accuracy: 0.8067\n",
      "Epoch 288/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5400 - accuracy: 0.8067\n",
      "Epoch 289/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5391 - accuracy: 0.8133\n",
      "Epoch 290/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5382 - accuracy: 0.8133\n",
      "Epoch 291/300\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.5373 - accuracy: 0.8133\n",
      "Epoch 292/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5364 - accuracy: 0.8133\n",
      "Epoch 293/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5355 - accuracy: 0.8133\n",
      "Epoch 294/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5346 - accuracy: 0.8133\n",
      "Epoch 295/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5336 - accuracy: 0.8200\n",
      "Epoch 296/300\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.5328 - accuracy: 0.8133\n",
      "Epoch 297/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5319 - accuracy: 0.8133\n",
      "Epoch 298/300\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.4907 - accuracy: 0.90 - 0s 60us/sample - loss: 0.5310 - accuracy: 0.8133\n",
      "Epoch 299/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5301 - accuracy: 0.8133\n",
      "Epoch 300/300\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.5293 - accuracy: 0.8133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28c6f628cc8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(scaled_X,y,epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"final_iris_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_scaler.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler,'iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_model = load_model('final_iris_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_scaler = joblib.load('iris_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flower_example = {\"sepal_length\":5.1,\n",
    "                 \"sepal_width\":3.5,\n",
    "                 \"petal_length\":1.4,\n",
    "                 \"petal_width\":0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_predictions(model, scaler, sample_json):\n",
    "    \n",
    "    \n",
    "    s_len = sample_json[\"sepal_length\"]\n",
    "    s_wid = sample_json[\"sepal_width\"]\n",
    "    p_len = sample_json[\"petal_width\"]\n",
    "    p_wid = sample_json[\"petal_width\"]\n",
    "    \n",
    "    flower = [[s_len, s_wid, p_len, p_wid]]\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)[0]\n",
    "    \n",
    "    return classes[class_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setosa'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_predictions(flower_model, flower_scaler, flower_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "\n",
    "flower_model = load_model(\"final_iris_model.h5\")\n",
    "flower_scaler = joblib.load(\"iris_scaler.pkl\")\n",
    "\n",
    "\n",
    "def return_prediction(model,scaler,sample_json):\n",
    "    \n",
    "    # For larger data features, you should probably write a for loop\n",
    "    # That builds out this array for you\n",
    "    \n",
    "    s_len = sample_json['sepal_length']\n",
    "    s_wid = sample_json['sepal_width']\n",
    "    p_len = sample_json['petal_length']\n",
    "    p_wid = sample_json['petal_width']\n",
    "    \n",
    "    flower = [[s_len,s_wid,p_len,p_wid]]\n",
    "    \n",
    "    flower = scaler.transform(flower)\n",
    "    \n",
    "    classes = np.array(['setosa', 'versicolor', 'virginica'])\n",
    "    \n",
    "    class_ind = model.predict_classes(flower)\n",
    "    \n",
    "    return classes[class_ind][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
